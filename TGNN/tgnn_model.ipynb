{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f866af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lachlan\\anaconda3\\envs\\gnn_lmsc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Lachlan\\anaconda3\\envs\\gnn_lmsc\\lib\\site-packages\\scipy\\__init__.py:173: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.18.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# define datatype\n",
    "if torch.cuda.is_available():\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    dtype_long = torch.cuda.LongTensor\n",
    "else:\n",
    "    dtype = torch.FloatTensor\n",
    "    dtype_long = torch.LongTensor\n",
    "\n",
    "# model formulation\n",
    "class GNN_LMSC_cell(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, units:int, gcn_type: str,\n",
    "                    batch_size: int,  # this entry is unnecessary, kept only for backward compatibility\n",
    "                    width=125, depth=4,):\n",
    "            super(GNN_LMSC_cell, self).__init__()\n",
    "\n",
    "            self.in_channels = in_channels\n",
    "            self.out_channels = out_channels\n",
    "            self.units = units\n",
    "            self.gcn_type = gcn_type\n",
    "            self.batch_size = batch_size  # not needed\n",
    "            self.depth = depth\n",
    "\n",
    "            start_dim = units + in_channels\n",
    "            inside_dim = start_dim\n",
    "            self.qb =  Quadratic_block(inside_dim, width, depth)\n",
    "        \n",
    "            if gcn_type == 'GCNConv':\n",
    "                self.gconv1 = GCN_block(inside_dim, inside_dim, layer = 1)\n",
    "                self.gconv2 = GCN_block(inside_dim, inside_dim, layer = 1)\n",
    "            \n",
    "            if self.depth>0:\n",
    "                inside_dim = width\n",
    "            else:\n",
    "                inside_dim = in_channels\n",
    "\n",
    "            self.fc_alpha = nn.Linear(inside_dim,units)\n",
    "            self.fc_beta = nn.Linear(inside_dim,units)\n",
    "\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.fill_(0)\n",
    "                    # m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: torch.FloatTensor,\n",
    "        edge_index: torch.LongTensor,\n",
    "        edge_weight: torch.FloatTensor = None,\n",
    "        H: torch.FloatTensor = None,\n",
    "        ) -> torch.FloatTensor:\n",
    "\n",
    "        # Input strain increment X-> [batch, seq_len, in_dim]\n",
    "        # Hidden state H -> [batch, node_number, hidden_dim]\n",
    "\n",
    "        h_t = H\n",
    "    \n",
    "        strain_norm = torch.norm(X[:,:,0:6],dim=2)\n",
    "        x_input = X.clone()\n",
    "        x_input[:,:,0:6] = (x_input[:,:,0:6].squeeze(1)/(strain_norm + 1e-15)).unsqueeze(1)\n",
    "\n",
    "        cat_input = torch.cat([x_input.repeat(1,h_t.shape[1],1),h_t], dim = 2)\n",
    "\n",
    "        # graph embedding\n",
    "        if self.gcn_type == 'GCNConv':\n",
    "            G_input1 = self.gconv1(cat_input, edge_index)\n",
    "            G_input1 = F.relu(G_input1) \n",
    "            G_input2 = self.gconv2(cat_input, edge_index)\n",
    "            G_input2 = F.relu(G_input2) \n",
    "        else:\n",
    "            G_input1 = cat_input\n",
    "            G_input2 = cat_input\n",
    "\n",
    "        G_input1 = torch.tanh(self.qb(G_input1))\n",
    "        G_input2 = torch.tanh(self.qb(G_input2))\n",
    "\n",
    "        alpha = torch.exp(self.fc_alpha(G_input1))\n",
    "        beta  = torch.tanh(self.fc_beta(G_input2))\n",
    "\n",
    "        exp_f =  torch.exp(- alpha * strain_norm.unsqueeze(2).repeat(1,1,self.units))\n",
    "        h = exp_f * (h_t  - beta) + beta\n",
    "\n",
    "        return h.squeeze(1)\n",
    "\n",
    "\n",
    "class GNN_LMSC_Model(nn.Module):\n",
    "    def __init__(self, args, output_depth=3):\n",
    "        super(GNN_LMSC_Model, self).__init__()\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.seq_len = args.seq_len\n",
    "        self.output_dim = args.node_output_dim\n",
    "        self.batch_size = args.batch_size\n",
    "        self.node_num = args.num_nodes\n",
    "        self.input_dim = args.node_input_dim\n",
    "        self.lmsc = GNN_LMSC_cell(args.node_input_dim, args.node_output_dim,\n",
    "                                   units= args.hidden_dim, gcn_type=args.GCN_type, batch_size=args.batch_size)\n",
    "        \n",
    "        if args.out_type == 'FCNN':\n",
    "            self.decoder = FCNN(args.layers, nn.ReLU)\n",
    "        else:\n",
    "            output_depth = args.out_depth\n",
    "            self.decoder = Quadratic_block(args.hidden_dim, args.node_output_dim, output_depth)\n",
    "\n",
    "    def forward(self, data):\n",
    "        '''\n",
    "        -Data structure\n",
    "            1. x: input strain increment\n",
    "            2. edge_index: grain connections in the format of adjacency list\n",
    "            3. init_ori: initial grain orientations [grain_number, 3] \n",
    "        '''\n",
    "        x, edge_index = data.x.type(dtype), data.edge_index\n",
    "        edge_index = edge_index.to(device)\n",
    "\n",
    "        # preprocessing for batch training\n",
    "        # Input strain increment X-> [batch, seq_len, feature_dim]\n",
    "        x = x.view(-1,self.seq_len, self.input_dim)\n",
    "        edge_index = edge_index[:,0:edge_index.size(1)//self.batch_size]\n",
    "\n",
    "        # Hidden state H -> [batch, node_number, hidden_dim]\n",
    "        # h0 = torch.zeros(x.size(0), self.node_num, self.hidden_dim)\\\n",
    "        #     .requires_grad_().to(device)\n",
    "        h0 = torch.zeros(\n",
    "            x.size(0), self.node_num, self.hidden_dim,\n",
    "            device=device\n",
    "        )\n",
    "        # hidden_out = torch.zeros(x.size(0), self.node_num, x.size(1),\\\n",
    "        #                           self.hidden_dim).requires_grad_().to(device)\n",
    "        hidden_out = torch.zeros(\n",
    "            x.size(0), self.node_num, x.size(1), self.hidden_dim,\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "        # Assign initial orientation to hidden state\n",
    "        h0[:,:,0:3] = data.init_ori.view(-1,self.node_num,3).to(device)\n",
    "        h_last = h0.clone()\n",
    "\n",
    "        # sequential prediciton\n",
    "        for i in range(x.size(1)):\n",
    "            x_input = x[:,i,:].unsqueeze(1).clone()  \n",
    "            h_last = self.lmsc(x_input, edge_index, H = h_last)\n",
    "            h_last = h_last.clone()\n",
    "            hidden_out[:,:,i,:] = h_last.squeeze(1)\n",
    "        \n",
    "        # decode the hidden state to Output feature matrix\n",
    "        # [batch, node_number, hidden_dim] -> [batch, node_number, out_dim]\n",
    "        hidden_out = self.decoder(hidden_out)\n",
    "        return hidden_out\n",
    "\n",
    "\n",
    "\n",
    "class Quadratic_block(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, depth):\n",
    "        super(Quadratic_block, self,).__init__()\n",
    "        self.modlist1 = nn.ModuleList()\n",
    "        self.modlist2 = nn.ModuleList()\n",
    "        self.depth = depth\n",
    "        for i in range(depth):\n",
    "            if i == depth-1:\n",
    "                self.modlist1.append(torch.nn.Linear(input_dim, out_dim, bias=False))\n",
    "                self.modlist2.append(torch.nn.Linear(input_dim, out_dim, bias=False))\n",
    "            else:\n",
    "                self.modlist1.append(torch.nn.Linear(input_dim, input_dim, bias=False))\n",
    "                self.modlist2.append(torch.nn.Linear(input_dim, input_dim, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        i = 0\n",
    "        for m in self.modlist1:\n",
    "            x1 = m(x)\n",
    "            m2 = self.modlist2[i]\n",
    "            x2 = m2(x)\n",
    "            i += 1\n",
    "            if i < self.depth:\n",
    "                x1 = torch.tanh(x1)\n",
    "                x2 = torch.tanh(x2)\n",
    "            x = x1 * x2\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCN_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, layer,\\\n",
    "                  improved=False, cached=False, add_self_loops=True):\n",
    "        super(GCN_block, self,).__init__()\n",
    "        self.modlist = nn.ModuleList()\n",
    "        self.layer = layer\n",
    "        for i in range(layer):\n",
    "            if i == 0:\n",
    "                self.modlist.append(GCNConv(in_channels, out_channels,\\\n",
    "                                             improved, cached, add_self_loops))\n",
    "            else:\n",
    "                self.modlist.append(GCNConv(out_channels, out_channels,\\\n",
    "                                             improved, cached, add_self_loops))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        i = 0\n",
    "        for m in self.modlist:\n",
    "            x = m(x, edge_index)\n",
    "            i += 1\n",
    "            if i < self.layer:\n",
    "                x = torch.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FCNN(torch.nn.Module):\n",
    "    def __init__(self, layers, activation):\n",
    "        super(FCNN, self).__init__()\n",
    "   \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1        \n",
    "        # set up layer order dict\n",
    "        self.activation = activation\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93ca14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# ----- dummy args -----\n",
    "args = SimpleNamespace(\n",
    "    hidden_dim=16,\n",
    "    seq_len=5,\n",
    "    node_output_dim=6,\n",
    "    batch_size=1,\n",
    "    num_nodes=4,\n",
    "    node_input_dim=6,\n",
    "    GCN_type='GCNConv',\n",
    "    out_type='Quadratic',\n",
    "    out_depth=2,\n",
    "    layers=[16, 16, 6]\n",
    ")\n",
    "\n",
    "# ----- dummy graph -----\n",
    "edge_index = torch.tensor([\n",
    "    [0, 1, 2, 3, 0, 1],\n",
    "    [1, 0, 3, 2, 2, 3]\n",
    "], dtype=torch.long)\n",
    "\n",
    "x = torch.randn(args.seq_len * args.node_input_dim)\n",
    "init_ori = torch.randn(args.num_nodes, 3)\n",
    "\n",
    "data = Data(\n",
    "    x=x,\n",
    "    edge_index=edge_index,\n",
    "    init_ori=init_ori\n",
    ")\n",
    "\n",
    "# ----- run -----\n",
    "model = GNN_LMSC_Model(args).to(device)\n",
    "out = model(data)\n",
    "\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc11961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 101, 100, 15)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9b88074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 101, 100, 15)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "h5_data = h5py.File('demo_dataset_PN8_sq100.hdf5')\n",
    "all_data = h5_data['data'][:]\n",
    "\n",
    "print(all_data.shape)\n",
    "\n",
    "def nx_to_edge_index(G):\n",
    "    # Ensure nodes are labeled 0..N-1\n",
    "    G = nx.convert_node_labels_to_integers(G, ordering=\"sorted\")\n",
    "\n",
    "    # Get edge list\n",
    "    edges = list(G.edges())\n",
    "\n",
    "    # Make edges bidirectional\n",
    "    row = []\n",
    "    col = []\n",
    "    for i, j in edges:\n",
    "        row += [i, j]\n",
    "        col += [j, i]\n",
    "\n",
    "    edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "    return edge_index\n",
    "\n",
    "with open(\"RVE24-n100_edge_feature.pickle\", \"rb\") as f:\n",
    "    edges = pickle.load(f)\n",
    "\n",
    "edge_index_tensor = nx_to_edge_index(edges) # edge index tensor\n",
    "\n",
    "stress = all_data[:, :100, :, 3:9]   # stress from grains\n",
    "sigma_min = stress.min()\n",
    "sigma_max = stress.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de739114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_stress(stress, sigma_min, sigma_max):\n",
    "    return (2 * (stress - sigma_min) / (sigma_max - sigma_min)) - 1\n",
    "\n",
    "def denormalize_stress(stress_norm, sigma_min, sigma_max):\n",
    "    return (stress_norm + 1) * (sigma_max - sigma_min) / 2 + sigma_min\n",
    "\n",
    "def create_data_object(strain_path): # shape (node_num: grains, seq_len: strain increments, feature_dim: variables)\n",
    "    \"\"\"\n",
    "        Inputs\n",
    "    \"\"\"\n",
    "    init_ori = strain_path[:100, 0, 0:3]     # initial orientation of all 100 grains, excluded 101th grain for homogenized RVE\n",
    "    init_ori = np.deg2rad(init_ori)    # convert to radians\n",
    "    init_ori = torch.tensor(init_ori, dtype=torch.float32)\n",
    "\n",
    "    rve_acc_strain = strain_path[100, :, 9:15]    # accumulated strain  of homogenized RVE. Scaled by 100, following the paper.\n",
    "    strain_incs = np.zeros((100, 6), dtype=np.float32)\n",
    "    strain_incs[1:] = np.diff(rve_acc_strain, axis=0)    # calculating strain increment, from accumulated strain. First element remains as 0.\n",
    "    strain_incs *= 100    # Output multiplied by 100, follows the paper.\n",
    "    strain_incs = torch.tensor(strain_incs, dtype=torch.float32)\n",
    "\n",
    "    edge_index = edge_index_tensor\n",
    "\n",
    "    data = Data(\n",
    "        x=strain_incs,       # shape (seq_len, 6)\n",
    "        edge_index=edge_index,\n",
    "        init_ori=init_ori    # shape (100,3)\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_ground_truth(strain_path, sigma_min, sigma_max):\n",
    "    \"\"\"\n",
    "    Returns normalized ground truth\n",
    "    \"\"\"\n",
    "    y = np.copy(strain_path[:100, :, :])        # (100, 100, 15), omit 101th grain (RVE homogenized)\n",
    "\n",
    "    y[:, :, 0:3] = np.deg2rad(y[:, :, 0:3]) # convert orientation to radians\n",
    "\n",
    "    # normalize stress channels only (3:9)\n",
    "    y_stress = y[:, :, 3:9]\n",
    "    y[:, :, 3:9] = normalize_stress(y_stress, sigma_min, sigma_max)\n",
    "\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "891ce703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_data = all_data[:6]   # first 6 paths\n",
    "test_data  = all_data[6:]   # last 2 paths\n",
    "\n",
    "\n",
    "class StrainPathDataset(Dataset):\n",
    "    def __init__(self, data, sigma_min, sigma_max):\n",
    "        self.data = data    # loaded strain path from hdf5 demo dataset (paths, 101, 100, 15)\n",
    "        self.sigma_min = sigma_min\n",
    "        self.sigma_max = sigma_max\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        strain_path = self.data[idx]  # (101, 100, 15)\n",
    "\n",
    "        data = create_data_object(strain_path)\n",
    "        y = create_ground_truth(strain_path, self.sigma_min, self.sigma_max)\n",
    "\n",
    "        return data, y\n",
    "\n",
    "train_dataset = StrainPathDataset(train_data, sigma_min, sigma_max)\n",
    "test_dataset  = StrainPathDataset(test_data,  sigma_min, sigma_max)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74100a",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f26a40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    hidden_dim=64,\n",
    "    seq_len=100,\n",
    "    node_output_dim=15,     \n",
    "    batch_size=2,\n",
    "    num_nodes=100,\n",
    "    node_input_dim=6,\n",
    "    GCN_type='GCNConv',\n",
    "    out_type='Quadratic',   # NOT 'FCNN'\n",
    "    out_depth=3,\n",
    "    layers=None             # unused since out_type != 'FCNN'\n",
    ")\n",
    "\n",
    "model = GNN_LMSC_Model(args)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f69edca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.075757\n",
      "Epoch 1 | Loss: 0.073933\n",
      "Epoch 2 | Loss: 0.072669\n",
      "Epoch 3 | Loss: 0.071637\n",
      "Epoch 4 | Loss: 0.070752\n",
      "Epoch 5 | Loss: 0.069801\n",
      "Epoch 6 | Loss: 0.068816\n",
      "Epoch 7 | Loss: 0.068030\n",
      "Epoch 8 | Loss: 0.067131\n",
      "Epoch 9 | Loss: 0.066315\n",
      "Epoch 10 | Loss: 0.065637\n",
      "Epoch 11 | Loss: 0.064961\n",
      "Epoch 12 | Loss: 0.064274\n",
      "Epoch 13 | Loss: 0.063737\n",
      "Epoch 14 | Loss: 0.062985\n",
      "Epoch 15 | Loss: 0.062289\n",
      "Epoch 16 | Loss: 0.061702\n",
      "Epoch 17 | Loss: 0.061030\n",
      "Epoch 18 | Loss: 0.060300\n",
      "Epoch 19 | Loss: 0.059786\n",
      "Epoch 20 | Loss: 0.059020\n",
      "Epoch 21 | Loss: 0.058273\n",
      "Epoch 22 | Loss: 0.057664\n",
      "Epoch 23 | Loss: 0.057319\n",
      "Epoch 24 | Loss: 0.056899\n",
      "Epoch 25 | Loss: 0.056432\n",
      "Epoch 26 | Loss: 0.056067\n",
      "Epoch 27 | Loss: 0.055688\n",
      "Epoch 28 | Loss: 0.055284\n",
      "Epoch 29 | Loss: 0.055004\n",
      "Epoch 30 | Loss: 0.054707\n",
      "Epoch 31 | Loss: 0.054462\n",
      "Epoch 32 | Loss: 0.054281\n",
      "Epoch 33 | Loss: 0.054017\n",
      "Epoch 34 | Loss: 0.053967\n",
      "Epoch 35 | Loss: 0.053588\n",
      "Epoch 36 | Loss: 0.053307\n",
      "Epoch 37 | Loss: 0.053018\n",
      "Epoch 38 | Loss: 0.052772\n",
      "Epoch 39 | Loss: 0.052691\n",
      "Epoch 40 | Loss: 0.052400\n",
      "Epoch 41 | Loss: 0.052199\n",
      "Epoch 42 | Loss: 0.052083\n",
      "Epoch 43 | Loss: 0.051905\n",
      "Epoch 44 | Loss: 0.051832\n",
      "Epoch 45 | Loss: 0.051682\n",
      "Epoch 46 | Loss: 0.051520\n",
      "Epoch 47 | Loss: 0.051445\n",
      "Epoch 48 | Loss: 0.051377\n",
      "Epoch 49 | Loss: 0.051130\n",
      "Epoch 50 | Loss: 0.051041\n",
      "Epoch 51 | Loss: 0.051037\n",
      "Epoch 52 | Loss: 0.050840\n",
      "Epoch 53 | Loss: 0.050700\n",
      "Epoch 54 | Loss: 0.050701\n",
      "Epoch 55 | Loss: 0.050557\n",
      "Epoch 56 | Loss: 0.050455\n",
      "Epoch 57 | Loss: 0.050357\n",
      "Epoch 58 | Loss: 0.050286\n",
      "Epoch 59 | Loss: 0.050274\n",
      "Epoch 60 | Loss: 0.050156\n",
      "Epoch 61 | Loss: 0.050046\n",
      "Epoch 62 | Loss: 0.050010\n",
      "Epoch 63 | Loss: 0.049853\n",
      "Epoch 64 | Loss: 0.049780\n",
      "Epoch 65 | Loss: 0.049754\n",
      "Epoch 66 | Loss: 0.049683\n",
      "Epoch 67 | Loss: 0.049591\n",
      "Epoch 68 | Loss: 0.049507\n",
      "Epoch 69 | Loss: 0.049449\n",
      "Epoch 70 | Loss: 0.049318\n",
      "Epoch 71 | Loss: 0.049224\n",
      "Epoch 72 | Loss: 0.049183\n",
      "Epoch 73 | Loss: 0.049217\n",
      "Epoch 74 | Loss: 0.049039\n",
      "Epoch 75 | Loss: 0.048957\n",
      "Epoch 76 | Loss: 0.048894\n",
      "Epoch 77 | Loss: 0.048862\n",
      "Epoch 78 | Loss: 0.048743\n",
      "Epoch 79 | Loss: 0.048730\n",
      "Epoch 80 | Loss: 0.048664\n",
      "Epoch 81 | Loss: 0.048577\n",
      "Epoch 82 | Loss: 0.048541\n",
      "Epoch 83 | Loss: 0.048472\n",
      "Epoch 84 | Loss: 0.048436\n",
      "Epoch 85 | Loss: 0.048402\n",
      "Epoch 86 | Loss: 0.048294\n",
      "Epoch 87 | Loss: 0.048223\n",
      "Epoch 88 | Loss: 0.048174\n",
      "Epoch 89 | Loss: 0.048135\n",
      "Epoch 90 | Loss: 0.048084\n",
      "Epoch 91 | Loss: 0.048040\n",
      "Epoch 92 | Loss: 0.047930\n",
      "Epoch 93 | Loss: 0.047899\n",
      "Epoch 94 | Loss: 0.047840\n",
      "Epoch 95 | Loss: 0.047876\n",
      "Epoch 96 | Loss: 0.047743\n",
      "Epoch 97 | Loss: 0.047723\n",
      "Epoch 98 | Loss: 0.047628\n",
      "Epoch 99 | Loss: 0.047604\n",
      "Epoch 100 | Loss: 0.047519\n",
      "Epoch 101 | Loss: 0.047475\n",
      "Epoch 102 | Loss: 0.047407\n",
      "Epoch 103 | Loss: 0.047517\n",
      "Epoch 104 | Loss: 0.047343\n",
      "Epoch 105 | Loss: 0.047295\n",
      "Epoch 106 | Loss: 0.047343\n",
      "Epoch 107 | Loss: 0.047257\n",
      "Epoch 108 | Loss: 0.047250\n",
      "Epoch 109 | Loss: 0.047124\n",
      "Epoch 110 | Loss: 0.047051\n",
      "Epoch 111 | Loss: 0.047122\n",
      "Epoch 112 | Loss: 0.047245\n",
      "Epoch 113 | Loss: 0.046993\n",
      "Epoch 114 | Loss: 0.047051\n",
      "Epoch 115 | Loss: 0.046934\n",
      "Epoch 116 | Loss: 0.046843\n",
      "Epoch 117 | Loss: 0.046806\n",
      "Epoch 118 | Loss: 0.046777\n",
      "Epoch 119 | Loss: 0.046723\n",
      "Epoch 120 | Loss: 0.046651\n",
      "Epoch 121 | Loss: 0.046736\n",
      "Epoch 122 | Loss: 0.046585\n",
      "Epoch 123 | Loss: 0.046569\n",
      "Epoch 124 | Loss: 0.046549\n",
      "Epoch 125 | Loss: 0.046564\n",
      "Epoch 126 | Loss: 0.046469\n",
      "Epoch 127 | Loss: 0.046393\n",
      "Epoch 128 | Loss: 0.046466\n",
      "Epoch 129 | Loss: 0.046314\n",
      "Epoch 130 | Loss: 0.046257\n",
      "Epoch 131 | Loss: 0.046264\n",
      "Epoch 132 | Loss: 0.046333\n",
      "Epoch 133 | Loss: 0.046448\n",
      "Epoch 134 | Loss: 0.046191\n",
      "Epoch 135 | Loss: 0.046145\n",
      "Epoch 136 | Loss: 0.046094\n",
      "Epoch 137 | Loss: 0.046052\n",
      "Epoch 138 | Loss: 0.046127\n",
      "Epoch 139 | Loss: 0.045967\n",
      "Epoch 140 | Loss: 0.046143\n",
      "Epoch 141 | Loss: 0.045925\n",
      "Epoch 142 | Loss: 0.045907\n",
      "Epoch 143 | Loss: 0.045851\n",
      "Epoch 144 | Loss: 0.045875\n",
      "Epoch 145 | Loss: 0.045833\n",
      "Epoch 146 | Loss: 0.045886\n",
      "Epoch 147 | Loss: 0.045757\n",
      "Epoch 148 | Loss: 0.045780\n",
      "Epoch 149 | Loss: 0.045726\n",
      "Epoch 150 | Loss: 0.045681\n",
      "Epoch 151 | Loss: 0.045649\n",
      "Epoch 152 | Loss: 0.045607\n",
      "Epoch 153 | Loss: 0.045555\n",
      "Epoch 154 | Loss: 0.045541\n",
      "Epoch 155 | Loss: 0.045563\n",
      "Epoch 156 | Loss: 0.045477\n",
      "Epoch 157 | Loss: 0.045470\n",
      "Epoch 158 | Loss: 0.045410\n",
      "Epoch 159 | Loss: 0.045359\n",
      "Epoch 160 | Loss: 0.045347\n",
      "Epoch 161 | Loss: 0.045492\n",
      "Epoch 162 | Loss: 0.045311\n",
      "Epoch 163 | Loss: 0.045329\n",
      "Epoch 164 | Loss: 0.045297\n",
      "Epoch 165 | Loss: 0.045227\n",
      "Epoch 166 | Loss: 0.045313\n",
      "Epoch 167 | Loss: 0.045211\n",
      "Epoch 168 | Loss: 0.045148\n",
      "Epoch 169 | Loss: 0.045149\n",
      "Epoch 170 | Loss: 0.045084\n",
      "Epoch 171 | Loss: 0.045121\n",
      "Epoch 172 | Loss: 0.045039\n",
      "Epoch 173 | Loss: 0.045019\n",
      "Epoch 174 | Loss: 0.044973\n",
      "Epoch 175 | Loss: 0.045050\n",
      "Epoch 176 | Loss: 0.044959\n",
      "Epoch 177 | Loss: 0.044904\n",
      "Epoch 178 | Loss: 0.044879\n",
      "Epoch 179 | Loss: 0.044871\n",
      "Epoch 180 | Loss: 0.044809\n",
      "Epoch 181 | Loss: 0.044891\n",
      "Epoch 182 | Loss: 0.044948\n",
      "Epoch 183 | Loss: 0.044761\n",
      "Epoch 184 | Loss: 0.044822\n",
      "Epoch 185 | Loss: 0.044783\n",
      "Epoch 186 | Loss: 0.044667\n",
      "Epoch 187 | Loss: 0.044665\n",
      "Epoch 188 | Loss: 0.044630\n",
      "Epoch 189 | Loss: 0.044663\n",
      "Epoch 190 | Loss: 0.044543\n",
      "Epoch 191 | Loss: 0.044585\n",
      "Epoch 192 | Loss: 0.044543\n",
      "Epoch 193 | Loss: 0.044526\n",
      "Epoch 194 | Loss: 0.044464\n",
      "Epoch 195 | Loss: 0.044483\n",
      "Epoch 196 | Loss: 0.044580\n",
      "Epoch 197 | Loss: 0.044454\n",
      "Epoch 198 | Loss: 0.044313\n",
      "Epoch 199 | Loss: 0.044298\n",
      "Epoch 200 | Loss: 0.044299\n",
      "Epoch 201 | Loss: 0.044321\n",
      "Epoch 202 | Loss: 0.044233\n",
      "Epoch 203 | Loss: 0.044175\n",
      "Epoch 204 | Loss: 0.044101\n",
      "Epoch 205 | Loss: 0.044058\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 17\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# loss = criterion(y_pred, y_true)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((y_pred \u001b[38;5;241m-\u001b[39m y_true)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lachlan\\anaconda3\\envs\\gnn_lmsc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[1], line 150\u001b[0m, in \u001b[0;36mGNN_LMSC_Model.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    149\u001b[0m     x_input \u001b[38;5;241m=\u001b[39m x[:,i,:]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mclone()  \n\u001b[1;32m--> 150\u001b[0m     h_last \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlmsc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mh_last\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     h_last \u001b[38;5;241m=\u001b[39m h_last\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    152\u001b[0m     hidden_out[:,:,i,:] \u001b[38;5;241m=\u001b[39m h_last\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lachlan\\anaconda3\\envs\\gnn_lmsc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[1], line 85\u001b[0m, in \u001b[0;36mGNN_LMSC_cell.forward\u001b[1;34m(self, X, edge_index, edge_weight, H)\u001b[0m\n\u001b[0;32m     82\u001b[0m     G_input2 \u001b[38;5;241m=\u001b[39m cat_input\n\u001b[0;32m     84\u001b[0m G_input1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqb(G_input1))\n\u001b[1;32m---> 85\u001b[0m G_input2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG_input2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     87\u001b[0m alpha \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_alpha(G_input1))\n\u001b[0;32m     88\u001b[0m beta  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_beta(G_input2))\n",
      "File \u001b[1;32mc:\\Users\\Lachlan\\anaconda3\\envs\\gnn_lmsc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[1], line 180\u001b[0m, in \u001b[0;36mQuadratic_block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    178\u001b[0m x1 \u001b[38;5;241m=\u001b[39m m(x)\n\u001b[0;32m    179\u001b[0m m2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodlist2[i]\n\u001b[1;32m--> 180\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[43mm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth:\n",
      "File \u001b[1;32mc:\\Users\\Lachlan\\anaconda3\\envs\\gnn_lmsc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Lachlan\\anaconda3\\envs\\gnn_lmsc\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for data, y_true in train_loader:\n",
    "        data = data.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(data)\n",
    "        # loss = criterion(y_pred, y_true)\n",
    "        loss = torch.mean((y_pred - y_true)**2)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    print(f\"Epoch {epoch} | Loss: {epoch_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0e9cbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orientation MSE: 0.18127737939357758\n",
      "Stress MSE: 0.10055667161941528\n",
      "Accumulated strain MSE: 0.006004763767123222\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_trues = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, y_true in test_loader:\n",
    "        data = data.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "\n",
    "        y_pred = model(data)\n",
    "\n",
    "        all_preds.append(y_pred.cpu())\n",
    "        all_trues.append(y_true.cpu())\n",
    "\n",
    "ori_mse = F.mse_loss(y_pred[..., 0:3], y_true[..., 0:3])\n",
    "stress_mse = F.mse_loss(y_pred[..., 3:9], y_true[..., 3:9])\n",
    "acc_strain_mse = F.mse_loss(y_pred[..., 9:15], y_true[..., 9:15])\n",
    "\n",
    "print(\"Orientation MSE:\", ori_mse.item())\n",
    "print(\"Stress MSE:\", stress_mse.item())\n",
    "print(\"Accumulated strain MSE:\", acc_strain_mse.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3aa7ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction orientation:  tensor([0.2828, 0.2823, 0.4492], device='cuda:0')\n",
      "Ground truth orientation:  tensor([3.8973, 0.3591, 1.0479], device='cuda:0')\n",
      "Original truth orientation:  [221.54156  20.53923  59.7123 ]\n",
      "\n",
      "\n",
      "Prediction Cauchy stress:  tensor([-0.0920,  0.0056, -0.0060,  0.0030, -0.0114,  0.0235], device='cuda:0')\n",
      "Ground truth Cauchy stress:  tensor([ 0.5557, -0.0793,  0.1501,  0.0526,  0.0274, -0.0987], device='cuda:0')\n",
      "Original truth Cauchy stress:  [232.73143 -39.71717  60.34594  19.38458   7.66089 -44.4092 ]\n",
      "\n",
      "\n",
      "Prediction accumulated strain:  tensor([-0.0201,  0.0246,  0.0153, -0.0002,  0.0009, -0.0051], device='cuda:0')\n",
      "Ground truth accumulated strain:  tensor([ 0.0896, -0.0267, -0.0496, -0.0158, -0.0099,  0.0013], device='cuda:0')\n",
      "Original truth accumulated strain:  [ 0.08904 -0.0275  -0.04836 -0.01587 -0.00992  0.0015 ]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b = 0         # batch index\n",
    "g = 31       # grain index\n",
    "t = 50         # timestep\n",
    "\n",
    "\n",
    "print(\"Prediction orientation: \", y_pred[b, g, t, 0:3])\n",
    "print(\"Ground truth orientation: \", y_true[b, g, t, 0:3])\n",
    "print('Original truth orientation: ', all_data[b+6, g, t, 0:3])\n",
    "print('\\n')\n",
    "\n",
    "print(\"Prediction Cauchy stress: \", y_pred[b, g, t, 3:9])\n",
    "print(\"Ground truth Cauchy stress: \", y_true[b, g, t, 3:9])\n",
    "print(\"Original truth Cauchy stress: \", all_data[b+6, g, t, 3:9])\n",
    "print('\\n')\n",
    "\n",
    "print(\"Prediction accumulated strain: \", y_pred[b, g, t, 9:15])\n",
    "print(\"Ground truth accumulated strain: \", y_true[b, g, t, 9:15])\n",
    "print('Original truth accumulated strain: ', all_data[b+6, g, t, 9:15])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244948b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac8ec64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-390.3163  -110.51343   82.76505   -2.18077   44.80113  -60.64523]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "h5_data = h5py.File('demo_dataset_PN8_sq100.hdf5')\n",
    "all_data = h5_data['data'][:]\n",
    "# print(all_data.shape)\n",
    "\n",
    "test_data = all_data[1]\n",
    "\n",
    "# orig_ori = test_data[g, t, ]\n",
    "print(test_data[g, t, ])\n",
    "\n",
    "# test = all_data[:100, :, :]        # (100, 100, 15)\n",
    "test = all_data[7, g, t, 3:9]        # (100, 100, 15)\n",
    "\n",
    "# normalize stress channels only (3:9)\n",
    "# test_stress = test[:, :, 3:9]\n",
    "normalised_stress = normalize_stress(test, sigma_min, sigma_max)\n",
    "\n",
    "# print(normalised_stress)\n",
    "# print(sigma_min, sigma_max)\n",
    "\n",
    "processed_data = create_data_object(test_data)\n",
    "# print(processed_data.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0439bf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 100, 15])\n",
      "torch.Size([2, 100, 100, 15])\n"
     ]
    }
   ],
   "source": [
    "from torch import device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data, y_true = next(iter(loader))\n",
    "data = data.to(device)\n",
    "y_true = y_true.to(device)\n",
    "\n",
    "y_pred = model(data)\n",
    "\n",
    "print(y_pred.shape)\n",
    "# print(data.x.shape)\n",
    "print(y_true.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fc02f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1504])\n",
      "0 2\n",
      "2 0\n",
      "0 12\n",
      "12 0\n",
      "0 13\n",
      "13 0\n",
      "0 17\n",
      "17 0\n",
      "0 30\n",
      "30 0\n",
      "0 34\n",
      "34 0\n",
      "0 40\n",
      "40 0\n",
      "0 46\n",
      "46 0\n",
      "0 50\n",
      "50 0\n",
      "0 57\n",
      "57 0\n",
      "0 65\n",
      "65 0\n",
      "0 68\n",
      "68 0\n",
      "0 72\n",
      "72 0\n",
      "0 81\n",
      "81 0\n",
      "0 89\n",
      "89 0\n",
      "0 92\n",
      "92 0\n",
      "0 97\n",
      "97 0\n",
      "2 4\n",
      "4 2\n",
      "2 17\n",
      "17 2\n",
      "2 18\n",
      "18 2\n",
      "2 34\n",
      "34 2\n",
      "2 36\n",
      "36 2\n",
      "2 40\n",
      "40 2\n",
      "2 59\n",
      "59 2\n",
      "2 60\n",
      "60 2\n",
      "2 68\n",
      "68 2\n",
      "2 74\n",
      "74 2\n",
      "2 79\n",
      "79 2\n",
      "2 82\n",
      "82 2\n",
      "2 85\n",
      "85 2\n",
      "2 89\n",
      "89 2\n",
      "2 98\n",
      "98 2\n",
      "12 13\n",
      "13 12\n",
      "12 15\n",
      "15 12\n",
      "12 30\n",
      "30 12\n",
      "12 31\n",
      "31 12\n",
      "12 37\n",
      "37 12\n",
      "12 38\n",
      "38 12\n",
      "12 57\n",
      "57 12\n",
      "12 61\n",
      "61 12\n",
      "12 72\n",
      "72 12\n",
      "12 80\n",
      "80 12\n",
      "12 91\n",
      "91 12\n",
      "12 95\n",
      "95 12\n",
      "12 97\n",
      "97 12\n",
      "13 5\n",
      "5 13\n",
      "13 37\n",
      "37 13\n",
      "13 38\n",
      "38 13\n",
      "13 46\n",
      "46 13\n",
      "13 57\n",
      "57 13\n",
      "13 58\n",
      "58 13\n",
      "13 68\n",
      "68 13\n",
      "13 81\n",
      "81 13\n",
      "13 85\n",
      "85 13\n",
      "13 92\n",
      "92 13\n",
      "13 97\n",
      "97 13\n",
      "13 99\n",
      "99 13\n",
      "17 18\n",
      "18 17\n",
      "17 24\n",
      "24 17\n",
      "17 34\n",
      "34 17\n",
      "17 48\n",
      "48 17\n",
      "17 50\n",
      "50 17\n",
      "17 65\n",
      "65 17\n",
      "17 68\n",
      "68 17\n",
      "17 74\n",
      "74 17\n",
      "17 82\n",
      "82 17\n",
      "30 11\n",
      "11 30\n",
      "30 26\n",
      "26 30\n",
      "30 31\n",
      "31 30\n",
      "30 39\n",
      "39 30\n",
      "30 50\n",
      "50 30\n",
      "30 57\n",
      "57 30\n",
      "30 61\n",
      "61 30\n",
      "30 72\n",
      "72 30\n",
      "30 76\n",
      "76 30\n",
      "30 78\n",
      "78 30\n",
      "30 91\n",
      "91 30\n",
      "30 98\n",
      "98 30\n",
      "34 4\n",
      "4 34\n",
      "34 32\n",
      "32 34\n",
      "34 37\n",
      "37 34\n",
      "34 40\n",
      "40 34\n",
      "34 50\n",
      "50 34\n",
      "34 56\n",
      "56 34\n",
      "34 59\n",
      "59 34\n",
      "34 60\n",
      "60 34\n",
      "34 61\n",
      "61 34\n",
      "34 72\n",
      "72 34\n",
      "34 74\n",
      "74 34\n",
      "34 85\n",
      "85 34\n",
      "34 89\n",
      "89 34\n",
      "34 90\n",
      "90 34\n",
      "40 4\n",
      "4 40\n",
      "40 7\n",
      "7 40\n",
      "40 8\n",
      "8 40\n",
      "40 23\n",
      "23 40\n",
      "40 36\n",
      "36 40\n",
      "40 51\n",
      "51 40\n",
      "40 56\n",
      "56 40\n",
      "40 60\n",
      "60 40\n",
      "40 62\n",
      "62 40\n",
      "40 67\n",
      "67 40\n",
      "40 72\n",
      "72 40\n",
      "40 87\n",
      "87 40\n",
      "40 89\n",
      "89 40\n",
      "46 6\n",
      "6 46\n",
      "46 8\n",
      "8 46\n",
      "46 19\n",
      "19 46\n",
      "46 20\n",
      "20 46\n",
      "46 36\n",
      "36 46\n",
      "46 37\n",
      "37 46\n",
      "46 68\n",
      "68 46\n",
      "46 81\n",
      "81 46\n",
      "46 85\n",
      "85 46\n",
      "46 89\n",
      "89 46\n",
      "46 94\n",
      "94 46\n",
      "46 95\n",
      "95 46\n",
      "46 97\n",
      "97 46\n",
      "46 98\n",
      "98 46\n",
      "50 4\n",
      "4 50\n",
      "50 11\n",
      "11 50\n",
      "50 26\n",
      "26 50\n",
      "50 29\n",
      "29 50\n",
      "50 32\n",
      "32 50\n",
      "50 53\n",
      "53 50\n",
      "50 61\n",
      "61 50\n",
      "50 65\n",
      "65 50\n",
      "50 72\n",
      "72 50\n",
      "50 74\n",
      "74 50\n",
      "50 92\n",
      "92 50\n",
      "57 5\n",
      "5 57\n",
      "57 26\n",
      "26 57\n",
      "57 38\n",
      "38 57\n",
      "57 45\n",
      "45 57\n",
      "57 49\n",
      "49 57\n",
      "57 91\n",
      "91 57\n",
      "57 92\n",
      "92 57\n",
      "57 97\n",
      "97 57\n",
      "57 99\n",
      "99 57\n",
      "65 5\n",
      "5 65\n",
      "65 26\n",
      "26 65\n",
      "65 29\n",
      "29 65\n",
      "65 43\n",
      "43 65\n",
      "65 48\n",
      "48 65\n",
      "65 51\n",
      "51 65\n",
      "65 53\n",
      "53 65\n",
      "65 68\n",
      "68 65\n",
      "65 74\n",
      "74 65\n",
      "65 92\n",
      "92 65\n",
      "68 5\n",
      "5 68\n",
      "68 16\n",
      "16 68\n",
      "68 18\n",
      "18 68\n",
      "68 22\n",
      "22 68\n",
      "68 24\n",
      "24 68\n",
      "68 39\n",
      "39 68\n",
      "68 43\n",
      "43 68\n",
      "68 48\n",
      "48 68\n",
      "68 73\n",
      "73 68\n",
      "68 76\n",
      "76 68\n",
      "68 81\n",
      "81 68\n",
      "68 98\n",
      "98 68\n",
      "72 4\n",
      "4 72\n",
      "72 7\n",
      "7 72\n",
      "72 31\n",
      "31 72\n",
      "72 52\n",
      "52 72\n",
      "72 54\n",
      "54 72\n",
      "72 61\n",
      "61 72\n",
      "72 62\n",
      "62 72\n",
      "72 66\n",
      "66 72\n",
      "72 80\n",
      "80 72\n",
      "72 89\n",
      "89 72\n",
      "72 95\n",
      "95 72\n",
      "72 97\n",
      "97 72\n",
      "72 98\n",
      "98 72\n",
      "81 5\n",
      "5 81\n",
      "81 6\n",
      "6 81\n",
      "81 16\n",
      "16 81\n",
      "81 20\n",
      "20 81\n",
      "81 58\n",
      "58 81\n",
      "81 64\n",
      "64 81\n",
      "81 69\n",
      "69 81\n",
      "81 70\n",
      "70 81\n",
      "81 76\n",
      "76 81\n",
      "81 82\n",
      "82 81\n",
      "81 85\n",
      "85 81\n",
      "81 94\n",
      "94 81\n",
      "81 98\n",
      "98 81\n",
      "89 6\n",
      "6 89\n",
      "89 8\n",
      "8 89\n",
      "89 23\n",
      "23 89\n",
      "89 36\n",
      "36 89\n",
      "89 62\n",
      "62 89\n",
      "89 67\n",
      "67 89\n",
      "89 95\n",
      "95 89\n",
      "89 97\n",
      "97 89\n",
      "89 98\n",
      "98 89\n",
      "92 1\n",
      "1 92\n",
      "92 5\n",
      "5 92\n",
      "92 7\n",
      "7 92\n",
      "92 26\n",
      "26 92\n",
      "92 38\n",
      "38 92\n",
      "92 43\n",
      "43 92\n",
      "92 45\n",
      "45 92\n",
      "92 51\n",
      "51 92\n",
      "92 67\n",
      "67 92\n",
      "92 91\n",
      "91 92\n",
      "97 15\n",
      "15 97\n",
      "97 37\n",
      "37 97\n",
      "97 95\n",
      "95 97\n",
      "1 7\n",
      "7 1\n",
      "1 22\n",
      "22 1\n",
      "1 25\n",
      "25 1\n",
      "1 26\n",
      "26 1\n",
      "1 45\n",
      "45 1\n",
      "1 51\n",
      "51 1\n",
      "1 52\n",
      "52 1\n",
      "1 56\n",
      "56 1\n",
      "1 63\n",
      "63 1\n",
      "1 66\n",
      "66 1\n",
      "1 78\n",
      "78 1\n",
      "1 80\n",
      "80 1\n",
      "1 88\n",
      "88 1\n",
      "1 96\n",
      "96 1\n",
      "7 4\n",
      "4 7\n",
      "7 45\n",
      "45 7\n",
      "7 51\n",
      "51 7\n",
      "7 52\n",
      "52 7\n",
      "7 54\n",
      "54 7\n",
      "7 56\n",
      "56 7\n",
      "7 62\n",
      "62 7\n",
      "7 66\n",
      "66 7\n",
      "7 67\n",
      "67 7\n",
      "7 80\n",
      "80 7\n",
      "7 88\n",
      "88 7\n",
      "22 3\n",
      "3 22\n",
      "22 9\n",
      "9 22\n",
      "22 16\n",
      "16 22\n",
      "22 24\n",
      "24 22\n",
      "22 25\n",
      "25 22\n",
      "22 39\n",
      "39 22\n",
      "22 43\n",
      "43 22\n",
      "22 48\n",
      "48 22\n",
      "22 51\n",
      "51 22\n",
      "22 71\n",
      "71 22\n",
      "22 73\n",
      "73 22\n",
      "22 76\n",
      "76 22\n",
      "22 78\n",
      "78 22\n",
      "22 79\n",
      "79 22\n",
      "22 96\n",
      "96 22\n",
      "25 9\n",
      "9 25\n",
      "25 16\n",
      "16 25\n",
      "25 23\n",
      "23 25\n",
      "25 33\n",
      "33 25\n",
      "25 36\n",
      "36 25\n",
      "25 43\n",
      "43 25\n",
      "25 45\n",
      "45 25\n",
      "25 47\n",
      "47 25\n",
      "25 52\n",
      "52 25\n",
      "25 66\n",
      "66 25\n",
      "25 73\n",
      "73 25\n",
      "25 78\n",
      "78 25\n",
      "25 79\n",
      "79 25\n",
      "25 88\n",
      "88 25\n",
      "25 94\n",
      "94 25\n",
      "25 98\n",
      "98 25\n",
      "26 11\n",
      "11 26\n",
      "26 29\n",
      "29 26\n",
      "26 45\n",
      "45 26\n",
      "26 51\n",
      "51 26\n",
      "26 63\n",
      "63 26\n",
      "26 78\n",
      "78 26\n",
      "26 91\n",
      "91 26\n",
      "45 3\n",
      "3 45\n",
      "45 33\n",
      "33 45\n",
      "45 35\n",
      "35 45\n",
      "45 38\n",
      "38 45\n",
      "45 49\n",
      "49 45\n",
      "45 62\n",
      "62 45\n",
      "45 66\n",
      "66 45\n",
      "45 67\n",
      "67 45\n",
      "45 78\n",
      "78 45\n",
      "45 80\n",
      "80 45\n",
      "45 84\n",
      "84 45\n",
      "45 91\n",
      "91 45\n",
      "51 36\n",
      "36 51\n",
      "51 43\n",
      "43 51\n",
      "51 53\n",
      "53 51\n",
      "51 56\n",
      "56 51\n",
      "51 59\n",
      "59 51\n",
      "51 60\n",
      "60 51\n",
      "51 67\n",
      "67 51\n",
      "51 71\n",
      "71 51\n",
      "51 79\n",
      "79 51\n",
      "51 87\n",
      "87 51\n",
      "52 9\n",
      "9 52\n",
      "52 18\n",
      "18 52\n",
      "52 54\n",
      "54 52\n",
      "52 61\n",
      "61 52\n",
      "52 66\n",
      "66 52\n",
      "52 79\n",
      "79 52\n",
      "52 86\n",
      "86 52\n",
      "52 88\n",
      "88 52\n",
      "52 93\n",
      "93 52\n",
      "52 98\n",
      "98 52\n",
      "56 4\n",
      "4 56\n",
      "56 54\n",
      "54 56\n",
      "56 59\n",
      "59 56\n",
      "56 60\n",
      "60 56\n",
      "56 63\n",
      "63 56\n",
      "56 83\n",
      "83 56\n",
      "56 87\n",
      "87 56\n",
      "56 88\n",
      "88 56\n",
      "56 90\n",
      "90 56\n",
      "63 11\n",
      "11 63\n",
      "63 28\n",
      "28 63\n",
      "63 29\n",
      "29 63\n",
      "63 49\n",
      "49 63\n",
      "63 55\n",
      "55 63\n",
      "63 78\n",
      "78 63\n",
      "63 83\n",
      "83 63\n",
      "63 87\n",
      "87 63\n",
      "63 88\n",
      "88 63\n",
      "63 96\n",
      "96 63\n",
      "66 3\n",
      "3 66\n",
      "66 21\n",
      "21 66\n",
      "66 31\n",
      "31 66\n",
      "66 33\n",
      "33 66\n",
      "66 47\n",
      "47 66\n",
      "66 62\n",
      "62 66\n",
      "66 69\n",
      "69 66\n",
      "66 80\n",
      "80 66\n",
      "66 94\n",
      "94 66\n",
      "66 98\n",
      "98 66\n",
      "78 3\n",
      "3 78\n",
      "78 11\n",
      "11 78\n",
      "78 16\n",
      "16 78\n",
      "78 39\n",
      "39 78\n",
      "78 76\n",
      "76 78\n",
      "78 91\n",
      "91 78\n",
      "78 96\n",
      "96 78\n",
      "80 15\n",
      "15 80\n",
      "80 21\n",
      "21 80\n",
      "80 31\n",
      "31 80\n",
      "80 62\n",
      "62 80\n",
      "80 95\n",
      "95 80\n",
      "88 9\n",
      "9 88\n",
      "88 28\n",
      "28 88\n",
      "88 54\n",
      "54 88\n",
      "88 83\n",
      "83 88\n",
      "88 86\n",
      "86 88\n",
      "88 90\n",
      "90 88\n",
      "88 93\n",
      "93 88\n",
      "88 96\n",
      "96 88\n",
      "96 9\n",
      "9 96\n",
      "96 11\n",
      "11 96\n",
      "96 14\n",
      "14 96\n",
      "96 24\n",
      "24 96\n",
      "96 28\n",
      "28 96\n",
      "96 35\n",
      "35 96\n",
      "96 39\n",
      "39 96\n",
      "96 41\n",
      "41 96\n",
      "96 48\n",
      "48 96\n",
      "96 71\n",
      "71 96\n",
      "96 84\n",
      "84 96\n",
      "4 54\n",
      "54 4\n",
      "4 59\n",
      "59 4\n",
      "4 60\n",
      "60 4\n",
      "4 61\n",
      "61 4\n",
      "4 90\n",
      "90 4\n",
      "18 9\n",
      "9 18\n",
      "18 20\n",
      "20 18\n",
      "18 24\n",
      "24 18\n",
      "18 27\n",
      "27 18\n",
      "18 39\n",
      "39 18\n",
      "18 44\n",
      "44 18\n",
      "18 59\n",
      "59 18\n",
      "18 60\n",
      "60 18\n",
      "18 61\n",
      "61 18\n",
      "18 79\n",
      "79 18\n",
      "18 82\n",
      "82 18\n",
      "18 85\n",
      "85 18\n",
      "18 86\n",
      "86 18\n",
      "18 93\n",
      "93 18\n",
      "18 98\n",
      "98 18\n",
      "36 6\n",
      "6 36\n",
      "36 23\n",
      "23 36\n",
      "36 33\n",
      "33 36\n",
      "36 43\n",
      "43 36\n",
      "36 60\n",
      "60 36\n",
      "36 67\n",
      "67 36\n",
      "36 79\n",
      "79 36\n",
      "36 94\n",
      "94 36\n",
      "36 98\n",
      "98 36\n",
      "59 19\n",
      "19 59\n",
      "59 27\n",
      "27 59\n",
      "59 53\n",
      "53 59\n",
      "59 60\n",
      "60 59\n",
      "59 71\n",
      "71 59\n",
      "59 77\n",
      "77 59\n",
      "59 83\n",
      "83 59\n",
      "59 85\n",
      "85 59\n",
      "59 87\n",
      "87 59\n",
      "59 90\n",
      "90 59\n",
      "60 9\n",
      "9 60\n",
      "60 27\n",
      "27 60\n",
      "60 71\n",
      "71 60\n",
      "60 79\n",
      "79 60\n",
      "60 87\n",
      "87 60\n",
      "74 14\n",
      "14 74\n",
      "74 29\n",
      "29 74\n",
      "74 32\n",
      "32 74\n",
      "74 48\n",
      "48 74\n",
      "74 53\n",
      "53 74\n",
      "74 55\n",
      "55 74\n",
      "74 58\n",
      "58 74\n",
      "74 71\n",
      "71 74\n",
      "74 82\n",
      "82 74\n",
      "74 85\n",
      "85 74\n",
      "74 99\n",
      "99 74\n",
      "79 9\n",
      "9 79\n",
      "79 43\n",
      "43 79\n",
      "79 98\n",
      "98 79\n",
      "82 14\n",
      "14 82\n",
      "82 20\n",
      "20 82\n",
      "82 24\n",
      "24 82\n",
      "82 42\n",
      "42 82\n",
      "82 44\n",
      "44 82\n",
      "82 48\n",
      "48 82\n",
      "82 58\n",
      "58 82\n",
      "82 64\n",
      "64 82\n",
      "82 70\n",
      "70 82\n",
      "82 85\n",
      "85 82\n",
      "82 86\n",
      "86 82\n",
      "82 99\n",
      "99 82\n",
      "85 19\n",
      "19 85\n",
      "85 20\n",
      "20 85\n",
      "85 27\n",
      "27 85\n",
      "85 32\n",
      "32 85\n",
      "85 37\n",
      "37 85\n",
      "85 58\n",
      "58 85\n",
      "85 90\n",
      "90 85\n",
      "85 99\n",
      "99 85\n",
      "98 6\n",
      "6 98\n",
      "98 31\n",
      "31 98\n",
      "98 39\n",
      "39 98\n",
      "98 61\n",
      "61 98\n",
      "98 76\n",
      "76 98\n",
      "98 93\n",
      "93 98\n",
      "98 94\n",
      "94 98\n",
      "3 16\n",
      "16 3\n",
      "3 31\n",
      "31 3\n",
      "3 33\n",
      "33 3\n",
      "3 35\n",
      "35 3\n",
      "3 38\n",
      "38 3\n",
      "3 41\n",
      "41 3\n",
      "3 64\n",
      "64 3\n",
      "3 70\n",
      "70 3\n",
      "3 73\n",
      "73 3\n",
      "3 75\n",
      "75 3\n",
      "3 76\n",
      "76 3\n",
      "3 84\n",
      "84 3\n",
      "3 91\n",
      "91 3\n",
      "3 94\n",
      "94 3\n",
      "16 5\n",
      "5 16\n",
      "16 33\n",
      "33 16\n",
      "16 43\n",
      "43 16\n",
      "16 58\n",
      "58 16\n",
      "16 73\n",
      "73 16\n",
      "16 75\n",
      "75 16\n",
      "16 76\n",
      "76 16\n",
      "31 15\n",
      "15 31\n",
      "31 38\n",
      "38 31\n",
      "31 42\n",
      "42 31\n",
      "31 61\n",
      "61 31\n",
      "31 64\n",
      "64 31\n",
      "31 69\n",
      "69 31\n",
      "31 76\n",
      "76 31\n",
      "31 91\n",
      "91 31\n",
      "31 94\n",
      "94 31\n",
      "31 95\n",
      "95 31\n",
      "33 5\n",
      "5 33\n",
      "33 10\n",
      "10 33\n",
      "33 23\n",
      "23 33\n",
      "33 41\n",
      "41 33\n",
      "33 43\n",
      "43 33\n",
      "33 47\n",
      "47 33\n",
      "33 58\n",
      "58 33\n",
      "33 62\n",
      "62 33\n",
      "33 64\n",
      "64 33\n",
      "33 67\n",
      "67 33\n",
      "33 70\n",
      "70 33\n",
      "33 73\n",
      "73 33\n",
      "33 75\n",
      "75 33\n",
      "33 76\n",
      "76 33\n",
      "35 11\n",
      "11 35\n",
      "35 14\n",
      "14 35\n",
      "35 32\n",
      "32 35\n",
      "35 38\n",
      "38 35\n",
      "35 39\n",
      "39 35\n",
      "35 41\n",
      "41 35\n",
      "35 42\n",
      "42 35\n",
      "35 44\n",
      "44 35\n",
      "35 49\n",
      "49 35\n",
      "35 55\n",
      "55 35\n",
      "35 62\n",
      "62 35\n",
      "35 70\n",
      "70 35\n",
      "35 75\n",
      "75 35\n",
      "35 84\n",
      "84 35\n",
      "35 91\n",
      "91 35\n",
      "35 99\n",
      "99 35\n",
      "38 5\n",
      "5 38\n",
      "38 11\n",
      "11 38\n",
      "38 15\n",
      "15 38\n",
      "38 37\n",
      "37 38\n",
      "38 42\n",
      "42 38\n",
      "38 49\n",
      "49 38\n",
      "38 55\n",
      "55 38\n",
      "38 64\n",
      "64 38\n",
      "38 70\n",
      "70 38\n",
      "38 91\n",
      "91 38\n",
      "38 99\n",
      "99 38\n",
      "41 9\n",
      "9 41\n",
      "41 10\n",
      "10 41\n",
      "41 14\n",
      "14 41\n",
      "41 21\n",
      "21 41\n",
      "41 27\n",
      "27 41\n",
      "41 28\n",
      "28 41\n",
      "41 47\n",
      "47 41\n",
      "41 48\n",
      "48 41\n",
      "41 62\n",
      "62 41\n",
      "41 70\n",
      "70 41\n",
      "41 71\n",
      "71 41\n",
      "41 75\n",
      "75 41\n",
      "41 84\n",
      "84 41\n",
      "64 14\n",
      "14 64\n",
      "64 42\n",
      "42 64\n",
      "64 58\n",
      "58 64\n",
      "64 69\n",
      "69 64\n",
      "64 70\n",
      "70 64\n",
      "64 75\n",
      "75 64\n",
      "64 76\n",
      "76 64\n",
      "64 91\n",
      "91 64\n",
      "64 94\n",
      "94 64\n",
      "70 14\n",
      "14 70\n",
      "70 20\n",
      "20 70\n",
      "70 42\n",
      "42 70\n",
      "70 44\n",
      "44 70\n",
      "70 58\n",
      "58 70\n",
      "70 69\n",
      "69 70\n",
      "70 75\n",
      "75 70\n",
      "70 76\n",
      "76 70\n",
      "70 91\n",
      "91 70\n",
      "70 94\n",
      "94 70\n",
      "73 43\n",
      "43 73\n",
      "73 75\n",
      "75 73\n",
      "73 76\n",
      "76 73\n",
      "75 10\n",
      "10 75\n",
      "75 14\n",
      "14 75\n",
      "75 23\n",
      "23 75\n",
      "75 47\n",
      "47 75\n",
      "75 58\n",
      "58 75\n",
      "76 39\n",
      "39 76\n",
      "76 91\n",
      "91 76\n",
      "76 94\n",
      "94 76\n",
      "84 11\n",
      "11 84\n",
      "84 21\n",
      "21 84\n",
      "84 28\n",
      "28 84\n",
      "84 47\n",
      "47 84\n",
      "84 49\n",
      "49 84\n",
      "84 62\n",
      "62 84\n",
      "84 83\n",
      "83 84\n",
      "94 6\n",
      "6 94\n",
      "94 47\n",
      "47 94\n",
      "94 69\n",
      "69 94\n",
      "54 61\n",
      "61 54\n",
      "54 90\n",
      "90 54\n",
      "54 93\n",
      "93 54\n",
      "61 11\n",
      "11 61\n",
      "61 32\n",
      "32 61\n",
      "61 39\n",
      "39 61\n",
      "61 42\n",
      "42 61\n",
      "61 44\n",
      "44 61\n",
      "61 90\n",
      "90 61\n",
      "61 93\n",
      "93 61\n",
      "90 8\n",
      "8 90\n",
      "90 15\n",
      "15 90\n",
      "90 19\n",
      "19 90\n",
      "90 21\n",
      "21 90\n",
      "90 28\n",
      "28 90\n",
      "90 32\n",
      "32 90\n",
      "90 37\n",
      "37 90\n",
      "90 42\n",
      "42 90\n",
      "90 83\n",
      "83 90\n",
      "90 86\n",
      "86 90\n",
      "90 93\n",
      "93 90\n",
      "90 95\n",
      "95 90\n",
      "5 23\n",
      "23 5\n",
      "5 43\n",
      "43 5\n",
      "5 49\n",
      "49 5\n",
      "5 55\n",
      "55 5\n",
      "5 58\n",
      "58 5\n",
      "5 67\n",
      "67 5\n",
      "5 77\n",
      "77 5\n",
      "5 99\n",
      "99 5\n",
      "23 6\n",
      "6 23\n",
      "23 8\n",
      "8 23\n",
      "23 10\n",
      "10 23\n",
      "23 19\n",
      "19 23\n",
      "23 43\n",
      "43 23\n",
      "23 47\n",
      "47 23\n",
      "23 58\n",
      "58 23\n",
      "23 67\n",
      "67 23\n",
      "23 77\n",
      "77 23\n",
      "43 48\n",
      "48 43\n",
      "43 53\n",
      "53 43\n",
      "43 58\n",
      "58 43\n",
      "43 67\n",
      "67 43\n",
      "43 71\n",
      "71 43\n",
      "49 8\n",
      "8 49\n",
      "49 11\n",
      "11 49\n",
      "49 28\n",
      "28 49\n",
      "49 55\n",
      "55 49\n",
      "49 62\n",
      "62 49\n",
      "49 67\n",
      "67 49\n",
      "49 77\n",
      "77 49\n",
      "49 83\n",
      "83 49\n",
      "49 99\n",
      "99 49\n",
      "55 11\n",
      "11 55\n",
      "55 29\n",
      "29 55\n",
      "55 53\n",
      "53 55\n",
      "55 58\n",
      "58 55\n",
      "55 71\n",
      "71 55\n",
      "55 77\n",
      "77 55\n",
      "55 83\n",
      "83 55\n",
      "55 87\n",
      "87 55\n",
      "55 99\n",
      "99 55\n",
      "58 10\n",
      "10 58\n",
      "58 14\n",
      "14 58\n",
      "58 71\n",
      "71 58\n",
      "58 77\n",
      "77 58\n",
      "58 99\n",
      "99 58\n",
      "67 8\n",
      "8 67\n",
      "67 10\n",
      "10 67\n",
      "67 62\n",
      "62 67\n",
      "67 77\n",
      "77 67\n",
      "77 8\n",
      "8 77\n",
      "77 10\n",
      "10 77\n",
      "77 19\n",
      "19 77\n",
      "77 27\n",
      "27 77\n",
      "77 71\n",
      "71 77\n",
      "77 83\n",
      "83 77\n",
      "77 87\n",
      "87 77\n",
      "99 11\n",
      "11 99\n",
      "99 29\n",
      "29 99\n",
      "99 32\n",
      "32 99\n",
      "99 37\n",
      "37 99\n",
      "6 8\n",
      "8 6\n",
      "6 19\n",
      "19 6\n",
      "6 20\n",
      "20 6\n",
      "6 47\n",
      "47 6\n",
      "6 69\n",
      "69 6\n",
      "8 19\n",
      "19 8\n",
      "8 37\n",
      "37 8\n",
      "8 62\n",
      "62 8\n",
      "8 83\n",
      "83 8\n",
      "8 95\n",
      "95 8\n",
      "19 10\n",
      "10 19\n",
      "19 20\n",
      "20 19\n",
      "19 27\n",
      "27 19\n",
      "19 47\n",
      "47 19\n",
      "19 83\n",
      "83 19\n",
      "20 21\n",
      "21 20\n",
      "20 27\n",
      "27 20\n",
      "20 47\n",
      "47 20\n",
      "20 69\n",
      "69 20\n",
      "20 86\n",
      "86 20\n",
      "47 10\n",
      "10 47\n",
      "47 21\n",
      "21 47\n",
      "47 27\n",
      "27 47\n",
      "47 62\n",
      "62 47\n",
      "47 69\n",
      "69 47\n",
      "69 15\n",
      "15 69\n",
      "69 21\n",
      "21 69\n",
      "69 42\n",
      "42 69\n",
      "69 44\n",
      "44 69\n",
      "69 86\n",
      "86 69\n",
      "62 21\n",
      "21 62\n",
      "62 28\n",
      "28 62\n",
      "62 83\n",
      "83 62\n",
      "62 95\n",
      "95 62\n",
      "37 15\n",
      "15 37\n",
      "37 32\n",
      "32 37\n",
      "37 42\n",
      "42 37\n",
      "37 95\n",
      "95 37\n",
      "83 28\n",
      "28 83\n",
      "83 87\n",
      "87 83\n",
      "83 95\n",
      "95 83\n",
      "95 15\n",
      "15 95\n",
      "95 21\n",
      "21 95\n",
      "95 28\n",
      "28 95\n",
      "9 21\n",
      "21 9\n",
      "9 27\n",
      "27 9\n",
      "9 28\n",
      "28 9\n",
      "9 48\n",
      "48 9\n",
      "9 71\n",
      "71 9\n",
      "9 86\n",
      "86 9\n",
      "21 15\n",
      "15 21\n",
      "21 27\n",
      "27 21\n",
      "21 28\n",
      "28 21\n",
      "21 42\n",
      "42 21\n",
      "21 86\n",
      "86 21\n",
      "27 10\n",
      "10 27\n",
      "27 71\n",
      "71 27\n",
      "27 86\n",
      "86 27\n",
      "28 11\n",
      "11 28\n",
      "28 86\n",
      "86 28\n",
      "48 10\n",
      "10 48\n",
      "48 14\n",
      "14 48\n",
      "48 24\n",
      "24 48\n",
      "48 53\n",
      "53 48\n",
      "48 71\n",
      "71 48\n",
      "71 10\n",
      "10 71\n",
      "71 14\n",
      "14 71\n",
      "71 53\n",
      "53 71\n",
      "71 87\n",
      "87 71\n",
      "86 15\n",
      "15 86\n",
      "86 42\n",
      "42 86\n",
      "86 44\n",
      "44 86\n",
      "86 93\n",
      "93 86\n",
      "10 14\n",
      "14 10\n",
      "14 24\n",
      "24 14\n",
      "14 39\n",
      "39 14\n",
      "14 42\n",
      "42 14\n",
      "14 44\n",
      "44 14\n",
      "11 29\n",
      "29 11\n",
      "11 32\n",
      "32 11\n",
      "11 39\n",
      "39 11\n",
      "11 44\n",
      "44 11\n",
      "29 32\n",
      "32 29\n",
      "29 53\n",
      "53 29\n",
      "29 87\n",
      "87 29\n",
      "32 42\n",
      "42 32\n",
      "32 44\n",
      "44 32\n",
      "39 24\n",
      "24 39\n",
      "39 44\n",
      "44 39\n",
      "39 93\n",
      "93 39\n",
      "44 24\n",
      "24 44\n",
      "44 42\n",
      "42 44\n",
      "44 93\n",
      "93 44\n",
      "15 42\n",
      "42 15\n",
      "42 93\n",
      "93 42\n",
      "53 87\n",
      "87 53\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "with open(\"RVE24-n100_edge_feature.pickle\", \"rb\") as f:\n",
    "    edges = pickle.load(f)\n",
    "\n",
    "\n",
    "def nx_to_edge_index(G):\n",
    "    # Ensure nodes are labeled 0..N-1\n",
    "    G = nx.convert_node_labels_to_integers(G, ordering=\"sorted\")\n",
    "\n",
    "    # Get edge list\n",
    "    edges = list(G.edges())\n",
    "\n",
    "    # Make edges bidirectional\n",
    "    row = []\n",
    "    col = []\n",
    "    for i, j in edges:\n",
    "        row += [i, j]\n",
    "        col += [j, i]\n",
    "\n",
    "    edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "    return edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d666a5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[228.0067   18.6711   15.5926 ]\n",
      " [227.94653  18.63262  15.63924]\n",
      " [227.88055  18.59222  15.68936]\n",
      " [227.80481  18.55232  15.74941]\n",
      " [227.7251   18.51262  15.81334]]\n",
      "[[3.9794676  0.32587218 0.2721422 ]\n",
      " [3.9784174  0.32520056 0.27295622]\n",
      " [3.9772658  0.32449546 0.27383098]\n",
      " [3.975944   0.32379907 0.27487904]\n",
      " [3.9745526  0.3231062  0.27599484]]\n"
     ]
    }
   ],
   "source": [
    "# data_obj = create_data_object(data[0])\n",
    "\n",
    "# print(data_obj.x.shape)\n",
    "\n",
    "# for batch in loader:\n",
    "#     print(\"x:\", batch.x.shape)\n",
    "#     print(\"edge_index:\", batch.edge_index.shape)\n",
    "#     print(\"init_ori:\", batch.init_ori.shape)\n",
    "#     print(\"batch vector:\", batch.batch.shape)\n",
    "#     break\n",
    "\n",
    "print(path0[0, :5, 0:3])\n",
    "print(np.deg2rad(path0[0, :5, 0:3]))              # convert to radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ca45518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# for path in data:\n",
    "#     print(path[13, 0, 9:15])\n",
    "\n",
    "#     # print(len(path[0, :, 9:15]))\n",
    "\n",
    "# print(data[0, :, 0, 9:15])\n",
    "\n",
    "path0 = data[0]\n",
    "\n",
    "print(path0[:100, 0, 9:15])\n",
    "# print(np.diff(path0[100, :, 9:15][:3], axis=0))\n",
    "\n",
    "\n",
    "test = [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]]\n",
    "# print(np.diff(test))\n",
    "\n",
    "arr = np.array([[1, 2],\n",
    "                [2, 3]])\n",
    "\n",
    "# print(arr.shape)  # (2, 2)\n",
    "\n",
    "# print(np.diff(arr, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9039f617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
      "         72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
      "         90, 91, 92, 93, 94, 95, 96, 97, 98],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
      "         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
      "         55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
      "         73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90,\n",
      "         91, 92, 93, 94, 95, 96, 97, 98, 99]])\n"
     ]
    }
   ],
   "source": [
    "edge_index = torch.tensor([\n",
    "    [i for i in range(99) for _ in (0,)],\n",
    "    [i+1 for i in range(99)]\n",
    "], dtype=torch.long)\n",
    "\n",
    "print(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa54fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_lmsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
