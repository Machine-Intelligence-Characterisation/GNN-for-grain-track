{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c0d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define datatype\n",
    "if torch.cuda.is_available():\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    dtype_long = torch.cuda.LongTensor\n",
    "else:\n",
    "    dtype = torch.FloatTensor\n",
    "    dtype_long = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2c5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quadratic_block(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, depth):\n",
    "        super(Quadratic_block, self,).__init__()\n",
    "        self.modlist1 = nn.ModuleList()\n",
    "        self.modlist2 = nn.ModuleList()\n",
    "        self.depth = depth\n",
    "        for i in range(depth):\n",
    "            if i == depth-1:\n",
    "                self.modlist1.append(torch.nn.Linear(input_dim, out_dim, bias=False))\n",
    "                self.modlist2.append(torch.nn.Linear(input_dim, out_dim, bias=False))\n",
    "            else:\n",
    "                self.modlist1.append(torch.nn.Linear(input_dim, input_dim, bias=False))\n",
    "                self.modlist2.append(torch.nn.Linear(input_dim, input_dim, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        i = 0\n",
    "        for m in self.modlist1:\n",
    "            x1 = m(x)\n",
    "            m2 = self.modlist2[i]\n",
    "            x2 = m2(x)\n",
    "            i += 1\n",
    "            if i < self.depth:\n",
    "                x1 = torch.tanh(x1)\n",
    "                x2 = torch.tanh(x2)\n",
    "            x = x1 * x2\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCN_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, layer,\\\n",
    "                  improved=False, cached=False, add_self_loops=True):\n",
    "        super(GCN_block, self,).__init__()\n",
    "        self.modlist = nn.ModuleList()\n",
    "        self.layer = layer\n",
    "        for i in range(layer):\n",
    "            if i == 0:\n",
    "                self.modlist.append(GCNConv(in_channels, out_channels,\\\n",
    "                                             improved, cached, add_self_loops))\n",
    "            else:\n",
    "                self.modlist.append(GCNConv(out_channels, out_channels,\\\n",
    "                                             improved, cached, add_self_loops))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        i = 0\n",
    "        for m in self.modlist:\n",
    "            x = m(x, edge_index)\n",
    "            i += 1\n",
    "            if i < self.layer:\n",
    "                x = torch.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a711fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptedGNN_LMSC_cell(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, units:int, gcn_type: str,\n",
    "                    batch_size: int, width=125, depth=4):\n",
    "            super(AdaptedGNN_LMSC_cell, self).__init__()\n",
    "            \n",
    "            self.in_channels = in_channels\n",
    "            self.units = units\n",
    "            self.gcn_type = gcn_type\n",
    "            self.depth = depth\n",
    "            self.width = width\n",
    "\n",
    "            start_dim = units + in_channels\n",
    "            inside_dim = start_dim\n",
    "            self.qb = Quadratic_block(inside_dim, width, depth)\n",
    "        \n",
    "            if gcn_type == 'GCNConv':\n",
    "                self.gconv1 = GCN_block(inside_dim, inside_dim, layer=1)\n",
    "                self.gconv2 = GCN_block(inside_dim, inside_dim, layer=1)\n",
    "            \n",
    "            if self.depth > 0:\n",
    "                inside_dim = width\n",
    "            else:\n",
    "                inside_dim = in_channels\n",
    "\n",
    "            self.fc_alpha = nn.Linear(inside_dim, units)\n",
    "            self.fc_beta = nn.Linear(inside_dim, units)\n",
    "\n",
    "            # for m in self.modules():\n",
    "            #     if isinstance(m, nn.Linear):\n",
    "            #         torch.nn.init.xavier_uniform_(m.weight)\n",
    "            #         m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, X, edge_index, edge_weight=None, H=None):\n",
    "        \"\"\"\n",
    "        Modified forward pass for your feature structure\n",
    "        \"\"\"\n",
    "        h_t = H\n",
    "        \n",
    "        # REMOVE strain normalization - you don't have separate strain increments\n",
    "        # Your features are: [size, phase_id, orient1, orient2, orient3, strain_step]\n",
    "        \n",
    "        # Use all features as input (no strain decomposition)\n",
    "        x_input = X.clone()\n",
    "        \n",
    "        # Concatenate input with hidden state\n",
    "        cat_input = torch.cat([x_input, h_t], dim=-1)  # [batch, num_nodes, units + in_channels]\n",
    "\n",
    "        # Graph embedding\n",
    "        if self.gcn_type == 'GCNConv':\n",
    "            G_input1 = self.gconv1(cat_input, edge_index)\n",
    "            G_input1 = F.relu(G_input1) \n",
    "            G_input2 = self.gconv2(cat_input, edge_index)\n",
    "            G_input2 = F.relu(G_input2) \n",
    "        else:\n",
    "            G_input1 = cat_input\n",
    "            G_input2 = cat_input\n",
    "\n",
    "        G_input1 = torch.tanh(self.qb(G_input1))\n",
    "        G_input2 = torch.tanh(self.qb(G_input2))\n",
    "\n",
    "        alpha = torch.exp(self.fc_alpha(G_input1))\n",
    "        beta = torch.tanh(self.fc_beta(G_input2))\n",
    "\n",
    "        pseudo_strain_norm = 1.0  # Constant for single-step prediction\n",
    "        exp_f = torch.exp(-alpha * pseudo_strain_norm)\n",
    "        \n",
    "        h = exp_f * (h_t - beta) + beta\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88280ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleStepGrainPredictor(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SingleStepGrainPredictor, self).__init__()\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.output_dim = args.output_dim  \n",
    "        self.batch_size = args.batch_size\n",
    "        \n",
    "        # Input dimension matches your features: [size, phase_id, orient1, orient2, orient3, strain_step]\n",
    "        self.input_dim = args.input_dim  \n",
    "        \n",
    "        # Single LMSC cell for one-step prediction\n",
    "        self.lmsc = AdaptedGNN_LMSC_cell(\n",
    "            self.input_dim,  \n",
    "            self.hidden_dim,  # Output hidden states, not predictions\n",
    "            units=self.hidden_dim, \n",
    "            gcn_type=args.GCN_type, \n",
    "            batch_size=args.batch_size\n",
    "        )\n",
    "        \n",
    "        # Decoder to predict next grain state (excluding strain_step)\n",
    "        self.decoder = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        '''\n",
    "        Modified for single-step prediction with variable subgraphs\n",
    "        '''\n",
    "        x, edge_index = data.x.type(dtype), data.edge_index\n",
    "        edge_index = edge_index.to(device)\n",
    "\n",
    "        # Your data: x shape [batch_size * num_nodes, input_dim]\n",
    "        # Reshape to [batch_size, num_nodes, input_dim]\n",
    "        batch_size = data.num_graphs if hasattr(data, 'num_graphs') else 1\n",
    "        num_nodes = data.num_nodes\n",
    "        \n",
    "        x = x.view(batch_size, num_nodes, self.input_dim)\n",
    "\n",
    "        # Hidden state initialization\n",
    "        h0 = torch.zeros(batch_size, num_nodes, self.hidden_dim).to(device)\n",
    "        \n",
    "        # Initialize with current features (orientation part)\n",
    "        init_ori_size = data.init_ori.size(-1)  # This will be 4 in your case\n",
    "        h0[:, :, :init_ori_size] = data.init_ori.unsqueeze(1).expand(-1, num_nodes, -1)\n",
    "\n",
    "        # SINGLE step prediction (not sequence)\n",
    "        # Use current state as input to predict next state\n",
    "        h_next = self.lmsc(x, edge_index, H=h0)  # [batch, num_nodes, hidden_dim]\n",
    "        \n",
    "        # Extract only the target grain (always index 0 in your subgraph)\n",
    "        target_hidden = h_next[:, 0, :]  # [batch, hidden_dim]\n",
    "        \n",
    "        # Predict next grain state (excluding strain_step)\n",
    "        next_state = self.decoder(target_hidden)  # [batch, output_dim]\n",
    "        \n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb1f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# all_data_points = torch.load('grain_data.pt')\n",
    "all_data_points = torch.load('grain_data_normalised.pt')\n",
    "\n",
    "train_data, temp_data = train_test_split(all_data_points, test_size=0.3, random_state=42)\n",
    "val_data, test_data   = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "val_loader   = DataLoader(val_data, batch_size=1, shuffle=False)\n",
    "test_loader  = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0cf59f",
   "metadata": {},
   "source": [
    "Removed Strain Step Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25917983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with validation and early stopping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lachlan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Define your model and configuration\n",
    "class Args:\n",
    "    hidden_dim = 250\n",
    "    input_dim = 7    # [size, avg_shear, phase_id, orient1, orient2, orient3]\n",
    "    output_dim = 7   # [size, avg_shear, phase_id, orient1, orient2, orient3]\n",
    "    batch_size = 1\n",
    "    GCN_type = 'GCNConv'\n",
    "\n",
    "args = Args()\n",
    "model = SingleStepGrainPredictor(args).to(device)\n",
    "\n",
    "s_size = torch.nn.Parameter(torch.tensor(0.0, device=device))\n",
    "s_shear = torch.nn.Parameter(torch.tensor(0.0, device=device))\n",
    "s_orient = torch.nn.Parameter(torch.tensor(0.0, device=device))\n",
    "s_phase = torch.nn.Parameter(torch.tensor(0.0, device=device))\n",
    "\n",
    "loss_fn_size = nn.MSELoss()\n",
    "loss_fn_shear = nn.MSELoss()\n",
    "loss_fn_phase = nn.BCEWithLogitsLoss()\n",
    "loss_fn_orient = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(list(model.parameters()), lr=0.001)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_data, batch_size=args.batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_data, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "# --- 2. Define helper: loss computation ---\n",
    "def compute_losses(predictions, batch):\n",
    "    target_size = batch.y[:, 0]\n",
    "    target_shear = batch.y[:, 1]\n",
    "    target_phase = batch.y[:, 2]\n",
    "    target_phase_binary = target_phase - 1.0\n",
    "    target_orientation = batch.y[:, 3:7]\n",
    "\n",
    "    pred_size = predictions[:, 0]\n",
    "    pred_shear = predictions[:, 1]\n",
    "    pred_phase_logit = predictions[:, 2]\n",
    "    pred_orientation = predictions[:, 3:7]\n",
    "\n",
    "    loss_size   = loss_fn_size(pred_size.squeeze(), target_size)\n",
    "    loss_shear  = loss_fn_shear(pred_shear, target_shear)\n",
    "    loss_orient = loss_fn_orient(pred_orientation, target_orientation)\n",
    "    loss_phase  = loss_fn_phase(pred_phase_logit, target_phase_binary)\n",
    "\n",
    "    total = loss_size + loss_shear + loss_orient + loss_phase\n",
    "\n",
    "    return total, (loss_size, loss_shear, loss_phase, loss_orient)\n",
    "\n",
    "\n",
    "# --- 3. Training + validation loop with early stopping ---\n",
    "def train_val_model(model, train_loader, val_loader, optimizer, num_epochs=100, patience=10):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    avg_losses, size_losses, shear_losses, phase_losses, orient_losses, val_losses = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_loss_size = 0\n",
    "        total_loss_shear = 0\n",
    "        total_loss_phase = 0\n",
    "        total_loss_orient = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch)\n",
    "            loss, (l_size, l_shear, l_phase, l_orient) = compute_losses(predictions, batch)\n",
    "            # print(loss.item())\n",
    "            # print(f'Losses: {l_size.item()}, {l_shear.item()}, {l_phase.item()}, {l_orient.item()}')\n",
    "            # print(f'Scales: {s_size.item()}, {s_shear.item()}, {s_phase.item()}, {s_orient.item()}')\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_loss_size += (torch.exp(-s_size) * l_size).item() \n",
    "            # print(((torch.exp(-s_size) * l_size) + s_size).item())\n",
    "            total_loss_shear += (torch.exp(-s_shear) * l_shear).item() \n",
    "            # print((torch.exp(-s_shear) * l_shear).item() + s_shear)\n",
    "            total_loss_phase += (torch.exp(-s_phase) * l_phase).item() \n",
    "            # print((torch.exp(-s_shear) * l_shear).item() + s_shear)\n",
    "            total_loss_orient += (torch.exp(-s_orient) * l_orient).item() \n",
    "            # print((torch.exp(-s_orient) * l_orient).item() + s_orient)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_loss_size = total_loss_size / len(train_loader)\n",
    "        avg_loss_shear = total_loss_shear / len(train_loader)\n",
    "        avg_loss_phase = total_loss_phase / len(train_loader)\n",
    "        avg_loss_orient = total_loss_orient / len(train_loader)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                predictions = model(batch)\n",
    "                loss, _ = compute_losses(predictions, batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # --- Store metrics ---\n",
    "        avg_losses.append(avg_loss)\n",
    "        size_losses.append(avg_loss_size)\n",
    "        shear_losses.append(avg_loss_shear)\n",
    "        phase_losses.append(avg_loss_phase)\n",
    "        orient_losses.append(avg_loss_orient)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # --- Print progress ---\n",
    "        print(f'Epoch {epoch:3d}, Train Loss: {avg_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "        print(f'  Size Loss: {avg_loss_size:.6f}')\n",
    "        print(f'  Shear Loss: {avg_loss_shear:.6f}')\n",
    "        print(f'  Phase Loss: {avg_loss_phase:.6f}')\n",
    "        print(f'  Orient Loss: {avg_loss_orient:.6f}')\n",
    "        # print(s_size, s_shear, s_phase, s_orient)\n",
    "        print('-' * 60)\n",
    "\n",
    "        # --- Early stopping check ---\n",
    "        if val_loss < best_val_loss - 1e-6:  # small tolerance\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}! Best Val Loss: {best_val_loss:.6f}\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    return model, avg_losses, size_losses, shear_losses, phase_losses, orient_losses, val_losses\n",
    "\n",
    "# --- 5. Run training ---\n",
    "print(\"Starting training with validation and early stopping...\")\n",
    "trained_model, avg_losses, size_losses, shear_losses, phase_losses, orient_losses, val_losses = \\\n",
    "    train_val_model(model, train_loader, val_loader, optimizer, num_epochs=20, patience=8)\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(trained_model, \"gnn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "153f1ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleStepGrainPredictor(\n",
       "  (lmsc): AdaptedGNN_LMSC_cell(\n",
       "    (qb): Quadratic_block(\n",
       "      (modlist1): ModuleList(\n",
       "        (0): Linear(in_features=257, out_features=257, bias=False)\n",
       "        (1): Linear(in_features=257, out_features=257, bias=False)\n",
       "        (2): Linear(in_features=257, out_features=257, bias=False)\n",
       "        (3): Linear(in_features=257, out_features=125, bias=False)\n",
       "      )\n",
       "      (modlist2): ModuleList(\n",
       "        (0): Linear(in_features=257, out_features=257, bias=False)\n",
       "        (1): Linear(in_features=257, out_features=257, bias=False)\n",
       "        (2): Linear(in_features=257, out_features=257, bias=False)\n",
       "        (3): Linear(in_features=257, out_features=125, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (gconv1): GCN_block(\n",
       "      (modlist): ModuleList(\n",
       "        (0): GCNConv(257, 257)\n",
       "      )\n",
       "    )\n",
       "    (gconv2): GCN_block(\n",
       "      (modlist): ModuleList(\n",
       "        (0): GCNConv(257, 257)\n",
       "      )\n",
       "    )\n",
       "    (fc_alpha): Linear(in_features=125, out_features=250, bias=True)\n",
       "    (fc_beta): Linear(in_features=125, out_features=250, bias=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=250, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "# model = torch.load(\"gnn_model.pth\")\n",
    "# model = torch.load(\"gnn_model.pth\", map_location=torch.device('cpu')) # no gpu\n",
    "model = torch.load(\"gnn_model_normalised.pth\", map_location=torch.device('cpu')) # no gpu\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf5a9cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lachlan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase label counts (true): {1: 2998, 2: 710}\n",
      "Phase label counts (pred): {1: 2824, 2: 884}\n",
      "\n",
      "==== Test Evaluation Results ====\n",
      "Total Weighted Test Loss: 0.199760\n",
      "Size  → MSE: 0.008899, MAE: 0.044323, R²: 0.0211\n",
      "Shear → MSE: 0.011764, MAE: 0.084537, R²: 0.0220\n",
      "Orient→ MSE: 0.056961, MAE: 0.116118, R²: 0.7396\n",
      "\n",
      "Phase Classification Metrics (labels 1/2):\n",
      "  Accuracy:  0.9531\n",
      "  Precision: 0.8032\n",
      "  Recall:    1.0000\n",
      "  F1 Score:  0.8908\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    # Collect metrics\n",
    "    all_true_phase = []\n",
    "    all_pred_phase = []\n",
    "    all_true_size = []\n",
    "    all_pred_size = []\n",
    "    all_true_shear = []\n",
    "    all_pred_shear = []\n",
    "    all_true_orient = []\n",
    "    all_pred_orient = []\n",
    "    \n",
    "    mse_size = 0\n",
    "    mse_shear = 0\n",
    "    mse_orient = 0\n",
    "    mae_size = 0\n",
    "    mae_shear = 0\n",
    "    mae_orient = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            predictions = model(batch)\n",
    "            loss, (loss_size, loss_shear, loss_phase, loss_orient) = compute_losses(predictions, batch)\n",
    "            test_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # --- Extract true & predicted values ---\n",
    "            true_size = batch.y[:, 0].cpu().numpy()\n",
    "            true_shear = batch.y[:, 1].cpu().numpy()\n",
    "            true_phase = batch.y[:, 2].cpu().numpy()  # stays 1 or 2\n",
    "            true_orient = batch.y[:, 3:7].cpu().numpy()\n",
    "            \n",
    "            pred_size = predictions[:, 0].cpu().numpy()\n",
    "            pred_shear = predictions[:, 1].cpu().numpy()\n",
    "            \n",
    "            phase_logit = predictions[:, 2]\n",
    "            phase_prob = torch.sigmoid(phase_logit)\n",
    "            pred_phase = (phase_prob > 0.5).float() + 1  # still 1 or 2\n",
    "            pred_orient = predictions[:, 3:7].cpu().numpy()\n",
    "\n",
    "            # --- Regression metrics ---\n",
    "            mse_size += np.mean((pred_size - true_size) ** 2)\n",
    "            mae_size += np.mean(np.abs(pred_size - true_size))\n",
    "            mse_shear += np.mean((pred_shear - true_shear) ** 2)\n",
    "            mae_shear += np.mean(np.abs(pred_shear - true_shear))\n",
    "            mse_orient += np.mean((pred_orient - true_orient) ** 2)\n",
    "            mae_orient += np.mean(np.abs(pred_orient - true_orient))\n",
    "\n",
    "            # --- Accumulate for R² computation ---\n",
    "            all_true_size.extend(true_size)\n",
    "            all_pred_size.extend(pred_size)\n",
    "            all_true_shear.extend(true_shear)\n",
    "            all_pred_shear.extend(pred_shear)\n",
    "            all_true_orient.extend(true_orient.flatten())\n",
    "            all_pred_orient.extend(pred_orient.flatten())\n",
    "\n",
    "            # --- Classification metrics ---\n",
    "            all_true_phase.extend(true_phase)\n",
    "            all_pred_phase.extend(pred_phase.cpu().numpy())\n",
    "\n",
    "    # --- Average regression metrics ---\n",
    "    test_loss /= num_batches\n",
    "    mse_size /= num_batches\n",
    "    mae_size /= num_batches\n",
    "    mse_shear /= num_batches\n",
    "    mae_shear /= num_batches\n",
    "    mse_orient /= num_batches\n",
    "    mae_orient /= num_batches\n",
    "\n",
    "    # --- Compute R² scores ---\n",
    "    r2_size = r2_score(all_true_size, all_pred_size)\n",
    "    r2_shear = r2_score(all_true_shear, all_pred_shear)\n",
    "    r2_orient = r2_score(all_true_orient, all_pred_orient)\n",
    "\n",
    "    # --- Classification scores (still using 1/2 labels) ---\n",
    "    all_true_phase = np.asarray(all_true_phase, dtype=int).ravel()\n",
    "    all_pred_phase = np.asarray(all_pred_phase, dtype=int).ravel()\n",
    "\n",
    "    # Sanity prints\n",
    "    print(\"Phase label counts (true):\", dict(zip(*np.unique(all_true_phase, return_counts=True))))\n",
    "    print(\"Phase label counts (pred):\", dict(zip(*np.unique(all_pred_phase, return_counts=True))))\n",
    "\n",
    "    phase_acc = accuracy_score(all_true_phase, all_pred_phase)\n",
    "    phase_prec = precision_score(all_true_phase, all_pred_phase, pos_label=2, zero_division=0)\n",
    "    phase_rec = recall_score(all_true_phase, all_pred_phase, pos_label=2, zero_division=0)\n",
    "    phase_f1 = f1_score(all_true_phase, all_pred_phase, pos_label=2, zero_division=0)\n",
    "\n",
    "    # --- Print summary ---\n",
    "    print(\"\\n==== Test Evaluation Results ====\")\n",
    "    print(f\"Total Weighted Test Loss: {test_loss:.6f}\")\n",
    "    print(f\"Size  → MSE: {mse_size:.6f}, MAE: {mae_size:.6f}, R²: {r2_size:.4f}\")\n",
    "    print(f\"Shear → MSE: {mse_shear:.6f}, MAE: {mae_shear:.6f}, R²: {r2_shear:.4f}\")\n",
    "    print(f\"Orient→ MSE: {mse_orient:.6f}, MAE: {mae_orient:.6f}, R²: {r2_orient:.4f}\")\n",
    "    print(\"\\nPhase Classification Metrics (labels 1/2):\")\n",
    "    print(f\"  Accuracy:  {phase_acc:.4f}\")\n",
    "    print(f\"  Precision: {phase_prec:.4f}\")\n",
    "    print(f\"  Recall:    {phase_rec:.4f}\")\n",
    "    print(f\"  F1 Score:  {phase_f1:.4f}\")\n",
    "    print('=' * 60)\n",
    "\n",
    "# evaluate_model(trained_model, test_loader)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "780f283a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# --- Plot total loss and each component ---\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mavg_losses\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(size_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSize Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(shear_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShear Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Plot total loss and each component ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(avg_losses, label='Total Loss', linewidth=2)\n",
    "plt.plot(size_losses, label='Size Loss', linestyle='--')\n",
    "plt.plot(shear_losses, label='Shear Loss', linestyle='--')\n",
    "plt.plot(phase_losses, label='Phase Loss', linestyle='--')\n",
    "plt.plot(orient_losses, label='Orientation Loss', linestyle='--')\n",
    "\n",
    "plt.title(\"Training Losses Across Epochs\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Epoch\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"Loss\", fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.legend(fontsize=10, frameon=True)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e2a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "with open(r\"..\\All Grain Data\\all_grains.pkl\", \"rb\") as f:\n",
    "    all_grains = pickle.load(f)\n",
    "\n",
    "with open(r\"..\\All Grain Data\\grain_property.pkl\", \"rb\") as f:\n",
    "    grain_property = pickle.load(f)\n",
    "    \n",
    "with open(\"..\\grain_tracking_graph_7steps.pkl\", \"rb\") as f:\n",
    "    linked_grain_graph = pickle.load(f)\n",
    "\n",
    "with open('..\\strain_grains.pkl', 'rb') as f:\n",
    "    strain_grains = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4527e",
   "metadata": {},
   "source": [
    "Compare input, model prediction, true output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e8132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sample: (4, 2409)\n",
      "Input Shear: 0.02307494208188692\n",
      "Input Size: 429\n",
      "Input Phase: 1\n",
      "nput Phase: [ 0.9309755  -0.14824287  0.06433457 -0.32736789]\n",
      "\n",
      "\n",
      "Next step: (5, 2448)\n",
      "True Shear: 0.02212937857689785\n",
      "True Size: 436\n",
      "True -hase: 1\n",
      "True Phase: [ 0.93124301 -0.14806403  0.06096297 -0.32733319]\n",
      "\n",
      "\n",
      "Prediction Grain size: 246.9119873046875\n",
      "Prediction Grain shear: 0.03876620531082153\n",
      "Prediction Phase: 1.0\n",
      "Prediction Orientations: [0.8718612790107727, -0.17640642821788788, 0.0202375166118145, -0.12630008161067963]\n"
     ]
    }
   ],
   "source": [
    "def get_data(step_and_id: tuple):\n",
    "    step, grain_id = step_and_id\n",
    "    positions = all_grains[step][grain_id]\n",
    "    property = grain_property[step][grain_id]\n",
    "    return positions, property\n",
    "\n",
    "\n",
    "def compare_predictions(index, model):\n",
    "    sample = all_data_points[index].to(device)\n",
    "\n",
    "    input_step = sample.current_step.item()\n",
    "    input_gid = sample.current_gid.item()\n",
    "    next_step = sample.next_step.item()\n",
    "    next_gid = sample.next_gid.item()\n",
    "\n",
    "    input_pos, input_prop = get_data((input_step, input_gid))\n",
    "    input_shear = strain_grains[input_step]['strains'][input_gid]\n",
    "    print(f'Input Sample: {input_step, input_gid}')\n",
    "    print(f'Input Shear: {input_shear}')\n",
    "    print(f'Input Size: {len(input_pos)}')\n",
    "    print(f\"Input Phase: {input_prop['phase_id']}\")\n",
    "    print(f\"nput Phase: {input_prop['average_orientation']}\")\n",
    "    print('\\n')\n",
    "\n",
    "    true_pos, true_prop = get_data((next_step, next_gid))\n",
    "    true_shear = strain_grains[next_step]['strains'][next_gid]\n",
    "    print(f'Next step: {next_step, next_gid}')\n",
    "    print(f'True Shear: {true_shear}')\n",
    "    print(f'True Size: {len(true_pos)}')\n",
    "    print(f\"True -hase: {true_prop['phase_id']}\")\n",
    "    print(f\"True Phase: {true_prop['average_orientation']}\")\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "    with torch.no_grad():  # disable gradient tracking\n",
    "        pred = model(sample)\n",
    "\n",
    "    # print(pred.tolist()[0])\n",
    "    pred_tensor = pred[0]  # Get first batch element if needed, but keep as tensor\n",
    "\n",
    "    grain_size = pred_tensor[0] * (5016-10) + 10\n",
    "    grain_shear = pred_tensor[1]\n",
    "    phase_logit = pred_tensor[2]\n",
    "    phase_prob = torch.sigmoid(phase_logit)\n",
    "    phase = (phase_prob > 0.5).float() + 1\n",
    "    orientations = pred_tensor[3:7]\n",
    "\n",
    "    grain_size_value = grain_size.item()\n",
    "    grain_shear = grain_shear.item()\n",
    "    phase_value = phase.item()\n",
    "    orientations_list = orientations.tolist()\n",
    "\n",
    "    print(f\"Prediction Grain size: {grain_size_value}\")\n",
    "    print(f\"Prediction Grain shear: {grain_shear}\")\n",
    "    print(f\"Prediction Phase: {phase_value}\")\n",
    "    print(f\"Prediction Orientations: {orientations_list}\")\n",
    "\n",
    "compare_predictions(254, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314607b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sample: (4, 2409)\n",
      "Input Shear: 0.02307494208188692\n",
      "Input Size: 429\n",
      "Input Phase: 1\n",
      "nput Phase: [ 0.9309755  -0.14824287  0.06433457 -0.32736789]\n",
      "\n",
      "\n",
      "Next step: (5, 2448)\n",
      "True Shear: 0.02212937857689785\n",
      "True Size: 436\n",
      "True -hase: 1\n",
      "True Phase: [ 0.93124301 -0.14806403  0.06096297 -0.32733319]\n",
      "\n",
      "\n",
      "Prediction Grain size: 0.05725782364606857\n",
      "Prediction Grain shear: 0.22193476557731628\n",
      "Prediction Phase: 1.0\n",
      "Prediction Orientations: [0.9125292301177979, -0.13527855277061462, 0.09346653521060944, -0.08168549090623856]\n"
     ]
    }
   ],
   "source": [
    "def get_data(step_and_id: tuple):\n",
    "    step, grain_id = step_and_id\n",
    "    positions = all_grains[step][grain_id]\n",
    "    property = grain_property[step][grain_id]\n",
    "    return positions, property\n",
    "\n",
    "\n",
    "def compare_predictions(index, model):\n",
    "    sample = all_data_points[index].to(device)\n",
    "\n",
    "    input_step = sample.current_step.item()\n",
    "    input_gid = sample.current_gid.item()\n",
    "    next_step = sample.next_step.item()\n",
    "    next_gid = sample.next_gid.item()\n",
    "\n",
    "    input_pos, input_prop = get_data((input_step, input_gid))\n",
    "    input_shear = strain_grains[input_step]['strains'][input_gid]\n",
    "    print(f'Input Sample: {input_step, input_gid}')\n",
    "    print(f'Input Shear: {input_shear}')\n",
    "    print(f'Input Size: {len(input_pos)}')\n",
    "    print(f\"Input Phase: {input_prop['phase_id']}\")\n",
    "    print(f\"nput Phase: {input_prop['average_orientation']}\")\n",
    "    print('\\n')\n",
    "\n",
    "    true_pos, true_prop = get_data((next_step, next_gid))\n",
    "    true_shear = strain_grains[next_step]['strains'][next_gid]\n",
    "    print(f'Next step: {next_step, next_gid}')\n",
    "    print(f'True Shear: {true_shear}')\n",
    "    print(f'True Size: {len(true_pos)}')\n",
    "    print(f\"True -hase: {true_prop['phase_id']}\")\n",
    "    print(f\"True Phase: {true_prop['average_orientation']}\")\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "    with torch.no_grad():  # disable gradient tracking\n",
    "        pred = model(sample)\n",
    "\n",
    "    # print(pred.tolist()[0])\n",
    "    pred_tensor = pred[0]  # Get first batch element if needed, but keep as tensor\n",
    "\n",
    "    # grain_size = pred_tensor[0] * (5016-10) + 10\n",
    "    grain_size = pred_tensor[0]\n",
    "    grain_shear = pred_tensor[1]\n",
    "    phase_logit = pred_tensor[2]\n",
    "    phase_prob = torch.sigmoid(phase_logit)\n",
    "    phase = (phase_prob > 0.5).float() + 1\n",
    "    orientations = pred_tensor[3:7]\n",
    "\n",
    "    grain_size_value = grain_size.item()\n",
    "    grain_shear = grain_shear.item()\n",
    "    phase_value = phase.item()\n",
    "    orientations_list = orientations.tolist()\n",
    "\n",
    "    print(f\"Prediction Grain size: {grain_size_value}\")\n",
    "    print(f\"Prediction Grain shear: {grain_shear}\")\n",
    "    print(f\"Prediction Phase: {phase_value}\")\n",
    "    print(f\"Prediction Orientations: {orientations_list}\")\n",
    "\n",
    "compare_predictions(254, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8955f1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor([ 0.0534,  0.1087,  1.0000,  0.1189, -0.0442, -0.0522, -0.9905])\n",
      "True: tensor([ 0.0554,  0.1668,  1.0000,  0.1204, -0.0466, -0.0507, -0.9903])\n",
      "Prediction: [0.05481639504432678, 0.21847331523895264, 1.0, [0.18501469492912292, -0.029368892312049866, -0.00817003846168518, 0.022139888256788254]]\n"
     ]
    }
   ],
   "source": [
    "def compare_predictions(index):\n",
    "    sample = all_data_points[index]\n",
    "\n",
    "    print(f\"Input {sample['x'][0]}\")\n",
    "    print(f\"True: {sample['y'][0]}\")\n",
    "\n",
    "    with torch.no_grad():  # disable gradient tracking\n",
    "        pred = model(sample)\n",
    "\n",
    "    pred_tensor = pred[0]\n",
    "\n",
    "    grain_size = pred_tensor[0]\n",
    "    grain_shear = pred_tensor[1]\n",
    "    phase_logit = pred_tensor[2]\n",
    "    phase_prob = torch.sigmoid(phase_logit)\n",
    "    phase = (phase_prob > 0.5).float() + 1\n",
    "    orientations = pred_tensor[3:7]\n",
    "\n",
    "    grain_size_value = grain_size.item()\n",
    "    grain_shear = grain_shear.item()\n",
    "    phase_value = phase.item()\n",
    "    orientations_list = orientations.tolist()\n",
    "\n",
    "    print(f\"Prediction: {[grain_size_value, grain_shear, phase_value, orientations_list]}\")\n",
    "\n",
    "compare_predictions(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d685c169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min size: 10\n",
      "Max size: 5016\n"
     ]
    }
   ],
   "source": [
    "min_size = float('inf')\n",
    "max_size = 0\n",
    "\n",
    "for sample in all_data_points:              # iterate over all data points\n",
    "\n",
    "    input_step = sample.current_step.item()\n",
    "    input_gid = sample.current_gid.item()\n",
    "    next_step = sample.next_step.item()\n",
    "    next_gid = sample.next_gid.item()\n",
    "\n",
    "    input_pos, input_prop = get_data((input_step, input_gid))\n",
    "    next_pos, next_prop = get_data((next_step, next_gid))\n",
    "\n",
    "    curr_size = len(input_pos)\n",
    "    next_size = len(next_pos)\n",
    "\n",
    "    # update global min/max\n",
    "    min_size = min(min_size, curr_size, next_size)\n",
    "    max_size = max(max_size, curr_size, next_size)\n",
    "\n",
    "print(\"Min size:\", min_size)\n",
    "print(\"Max size:\", max_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
